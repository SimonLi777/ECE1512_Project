{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZxi8JKMNs9B",
        "outputId": "a57ddc3f-22d1-429a-b87f-0b3f9658471a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/ECE1512')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helpful packages"
      ],
      "metadata": {
        "id": "SQchFMq7LYF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_h8ZMqI94xQ",
        "outputId": "d626cbde-6e61-4faf-bc3b-04c32f642f37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kornia\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/705.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/705.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m634.9/705.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (23.2)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.1->kornia) (1.3.0)\n",
            "Installing collected packages: kornia\n",
            "Successfully installed kornia-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VwAP7bJusH-_"
      },
      "outputs": [],
      "source": [
        "import networks\n",
        "import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms,datasets\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import torch.optim.lr_scheduler\n",
        "import copy\n",
        "from torchvision.utils import save_image\n",
        "from scheduler import LinearScheduler, CosineScheduler, ProportionScheduler\n",
        "from utils_gsam import get_network, get_daparam,\\\n",
        "    TensorDataset, epoch, ParamDiffAug\n",
        "#from gsam import GSAM\n",
        "\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "import contextlib\n",
        "from torch.distributed import ReduceOp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "def disable_running_stats(model):\n",
        "    def _disable(module):\n",
        "        if isinstance(module, _BatchNorm):\n",
        "            module.backup_momentum = module.momentum\n",
        "            module.momentum = 0\n",
        "\n",
        "    model.apply(_disable)\n",
        "\n",
        "def enable_running_stats(model):\n",
        "    def _enable(module):\n",
        "        if isinstance(module, _BatchNorm) and hasattr(module, \"backup_momentum\"):\n",
        "            module.momentum = module.backup_momentum\n",
        "\n",
        "    model.apply(_enable)"
      ],
      "metadata": {
        "id": "i-qngEaR8eyS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "#Ziyao Guo1, Kai Wang1, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You\n",
        "#https://github.com/GzyAftermath/DATM\n",
        "###############\n",
        "import torch\n",
        "import contextlib\n",
        "from torch.distributed import ReduceOp\n",
        "\n",
        "class GSAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, model, gsam_alpha, rho_scheduler, adaptive=False, perturb_eps=1e-12, grad_reduce='mean', **kwargs):\n",
        "        defaults = dict(adaptive=adaptive, **kwargs)\n",
        "        super(GSAM, self).__init__(params, defaults)\n",
        "        self.model = model\n",
        "        self.base_optimizer = base_optimizer\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "        self.adaptive = adaptive\n",
        "        self.rho_scheduler = rho_scheduler\n",
        "        self.perturb_eps = perturb_eps\n",
        "        self.alpha = gsam_alpha\n",
        "\n",
        "        # initialize self.rho_t\n",
        "        self.update_rho_t()\n",
        "\n",
        "        # set up reduction for gradient across workers\n",
        "        if grad_reduce.lower() == 'mean':\n",
        "            if hasattr(ReduceOp, 'AVG'):\n",
        "                self.grad_reduce = ReduceOp.AVG\n",
        "                self.manual_average = False\n",
        "            else: # PyTorch <= 1.11.0 does not have AVG, need to manually average across processes\n",
        "                self.grad_reduce = ReduceOp.SUM\n",
        "                self.manual_average = True\n",
        "        elif grad_reduce.lower() == 'sum':\n",
        "            self.grad_reduce = ReduceOp.SUM\n",
        "            self.manual_average = False\n",
        "        else:\n",
        "            raise ValueError('\"grad_reduce\" should be one of [\"mean\", \"sum\"].')\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_rho_t(self):\n",
        "        self.rho_t = self.rho_scheduler.step()\n",
        "        return self.rho_t\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def perturb_weights(self, rho=0.0):\n",
        "        grad_norm = self._grad_norm( weight_adaptive = self.adaptive )\n",
        "        for group in self.param_groups:\n",
        "            scale = rho / (grad_norm + self.perturb_eps)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_g\"] = p.grad.data.clone()\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                if self.adaptive:\n",
        "                    e_w *= torch.pow(p, 2)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p]['e_w'] = e_w\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def unperturb(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if 'e_w' in self.state[p].keys():\n",
        "                    p.data.sub_(self.state[p]['e_w'])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def gradient_decompose(self, alpha=0.0):\n",
        "        # calculate inner product\n",
        "        inner_prod = 0.0\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                inner_prod += torch.sum(\n",
        "                    self.state[p]['old_g'] * p.grad.data\n",
        "                )\n",
        "\n",
        "        # get norm\n",
        "        new_grad_norm = self._grad_norm()\n",
        "        old_grad_norm = self._grad_norm(by='old_g')\n",
        "\n",
        "        # get cosine\n",
        "        cosine = inner_prod / (new_grad_norm * old_grad_norm + self.perturb_eps)\n",
        "\n",
        "        # gradient decomposition\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                vertical = self.state[p]['old_g'] - cosine * old_grad_norm * p.grad.data / (new_grad_norm + self.perturb_eps)\n",
        "                p.grad.data.add_( vertical, alpha=-alpha)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _sync_grad(self):\n",
        "        if torch.distributed.is_initialized(): # synchronize final gardients\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    if p.grad is None: continue\n",
        "                    if self.manual_average:\n",
        "                        torch.distributed.all_reduce(p.grad, op=self.grad_reduce)\n",
        "                        world_size = torch.distributed.get_world_size()\n",
        "                        p.grad.div_(float(world_size))\n",
        "                    else:\n",
        "                        torch.distributed.all_reduce(p.grad, op=self.grad_reduce)\n",
        "        return\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _grad_norm(self, by=None, weight_adaptive=False):\n",
        "        #shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        if not by:\n",
        "            norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ( (torch.abs(p.data) if weight_adaptive else 1.0) *  p.grad).norm(p=2)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        else:\n",
        "            norm = torch.norm(\n",
        "                torch.stack([\n",
        "                    ( (torch.abs(p.data) if weight_adaptive else 1.0) * self.state[p][by]).norm(p=2)\n",
        "                    for group in self.param_groups for p in group[\"params\"]\n",
        "                    if p.grad is not None\n",
        "                ]),\n",
        "                p=2\n",
        "            )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n",
        "\n",
        "    def maybe_no_sync(self):\n",
        "        if torch.distributed.is_initialized():\n",
        "            return self.model.no_sync()\n",
        "        else:\n",
        "            return contextlib.ExitStack()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def set_closure(self, loss_fn, inputs, targets, **kwargs):\n",
        "        # create self.forward_backward_func, which is a function such that\n",
        "        # self.forward_backward_func() automatically performs forward and backward passes.\n",
        "        # This function does not take any arguments, and the inputs and targets data\n",
        "        # should be pre-set in the definition of partial-function\n",
        "\n",
        "        def get_grad():\n",
        "            self.base_optimizer.zero_grad()\n",
        "            with torch.enable_grad():\n",
        "                outputs = self.model(inputs)\n",
        "                loss = loss_fn(outputs, targets, **kwargs)\n",
        "            loss_value = loss.data.clone().detach()\n",
        "            loss.backward()\n",
        "            return outputs, loss_value\n",
        "\n",
        "        self.forward_backward_func = get_grad\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "\n",
        "        if closure:\n",
        "            get_grad = closure\n",
        "        else:\n",
        "            get_grad = self.forward_backward_func\n",
        "\n",
        "        with self.maybe_no_sync():\n",
        "            # get gradient\n",
        "            outputs, loss_value = get_grad()\n",
        "\n",
        "            # perturb weights\n",
        "            self.perturb_weights(rho=self.rho_t)\n",
        "\n",
        "            # disable running stats for second pass\n",
        "            disable_running_stats(self.model)\n",
        "\n",
        "            # get gradient at perturbed weights\n",
        "            get_grad()\n",
        "\n",
        "            # decompose and get new update direction\n",
        "            self.gradient_decompose(self.alpha)\n",
        "\n",
        "            # unperturb\n",
        "            self.unperturb()\n",
        "\n",
        "        # synchronize gradients across workers\n",
        "        self._sync_grad()\n",
        "\n",
        "        # update with new directions\n",
        "        self.base_optimizer.step()\n",
        "\n",
        "        # enable running stats\n",
        "        enable_running_stats(self.model)\n",
        "\n",
        "        return outputs, loss_value"
      ],
      "metadata": {
        "id": "fq15pTVB8cIr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trajectory Generation"
      ],
      "metadata": {
        "id": "PMLKc4KQLLvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "#Ziyao Guo1, Kai Wang1, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You\n",
        "#https://github.com/GzyAftermath/DATM\n",
        "###############\n",
        "def main():\n",
        "\n",
        "    #args.dsa = True if args.dsa == 'True' else False\n",
        "    #args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    #args.dsa_param = ParamDiffAug()\n",
        "\n",
        "    #channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv = get_dataset(\n",
        "    #    args.dataset, args.data_path, args.batch_real, args.subset, args=args)\n",
        "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader,loader_train_dict, class_map, class_map_inv = get_dataset(args.dataset, args.data_path, args.zca)\n",
        "    #channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(\n",
        "    #    args.dataset, args.data_path)\n",
        "    loader_train_dict, class_map, class_map_inv = None, None, None\n",
        "    class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "    # print('\\n================== Exp %d ==================\\n '%exp)\n",
        "    print('Hyper-parameters: \\n', args.__dict__)\n",
        "\n",
        "    #save_dir = os.path.join(args.buffer_path, args.dataset)\n",
        "    #if args.dataset == \"ImageNet\":\n",
        "    #    save_dir = os.path.join(save_dir, args.subset, str(args.res))\n",
        "    #if args.dataset in [\"CIFAR10\", \"CIFAR100\"] and not args.zca:\n",
        "    #    save_dir += \"_NO_ZCA\"\n",
        "    #save_dir = os.path.join(save_dir, args.model)\n",
        "    #if not os.path.exists(save_dir):\n",
        "    #    os.makedirs(save_dir)\n",
        "\n",
        "\n",
        "    save_dir = os.path.join(args.buffer_path, args.dataset)\n",
        "    save_dir = os.path.join(save_dir, args.model)\n",
        "    if not os.path.exists(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "\n",
        "    ''' organize the real dataset '''\n",
        "    images_all = []\n",
        "    labels_all = []\n",
        "    indices_class = [[] for c in range(num_classes)]\n",
        "    print(\"BUILDING DATASET\")\n",
        "    for i in tqdm(range(len(dst_train))):\n",
        "        sample = dst_train[i]\n",
        "        images_all.append(torch.unsqueeze(sample[0], dim=0))\n",
        "        labels_all.append(class_map[torch.tensor(sample[1]).item()])\n",
        "    #print('num of training images',len(images_all))\n",
        "    len_dst_train = len(images_all)  ##50000\n",
        "\n",
        "    for i, lab in tqdm(enumerate(labels_all)):\n",
        "        indices_class[lab].append(i)\n",
        "    images_all = torch.cat(images_all, dim=0).to(\"cpu\")\n",
        "    labels_all = torch.tensor(labels_all, dtype=torch.long, device=\"cpu\")\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
        "\n",
        "    for ch in range(channel):\n",
        "        print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(args.device)\n",
        "\n",
        "    trajectories = []\n",
        "\n",
        "    dst_train = TensorDataset(copy.deepcopy(images_all.detach()), copy.deepcopy(labels_all.detach()))\n",
        "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
        "\n",
        "    ''' set augmentation for whole-dataset training '''\n",
        "    args.dc_aug_param = get_daparam(args.dataset, args.model, args.model, None)\n",
        "    args.dc_aug_param['strategy'] = 'crop_scale_rotate'  # for whole-dataset training\n",
        "    print('DC augmentation parameters: \\n', args.dc_aug_param)\n",
        "\n",
        "    for it in range(0, args.num_experts):\n",
        "\n",
        "        ''' Train synthetic data '''\n",
        "        teacher_net = get_network(args.model, channel, num_classes, im_size).to(args.device) # get a random model\n",
        "        teacher_net.train()\n",
        "        lr = args.lr_teacher\n",
        "\n",
        "\n",
        "        ##modification: using FTD here\n",
        "        #from gsam import GSAM, LinearScheduler, CosineScheduler, ProportionScheduler\n",
        "        base_optimizer = torch.optim.SGD(teacher_net.parameters(), lr=lr, momentum=args.mom, weight_decay=args.l2)\n",
        "        # scheduler = CosineScheduler(T_max=args.train_epochs*len_dst_train, max_value=lr, min_value=0.0,\n",
        "            # optimizer=base_optimizer)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(base_optimizer,step_size=args.train_epochs*len(trainloader),gamma=1)\n",
        "        rho_scheduler = ProportionScheduler(pytorch_lr_scheduler=scheduler, max_lr=lr, min_lr=lr,\n",
        "            max_value=args.rho_max, min_value=args.rho_min)\n",
        "        teacher_optim = GSAM(params=teacher_net.parameters(), base_optimizer=base_optimizer,\n",
        "                model=teacher_net, gsam_alpha=args.alpha, rho_scheduler=rho_scheduler, adaptive=args.adaptive)\n",
        "\n",
        "\n",
        "        teacher_optim.zero_grad()\n",
        "\n",
        "        timestamps = []\n",
        "\n",
        "        timestamps.append([p.detach().cpu() for p in teacher_net.parameters()])\n",
        "\n",
        "        lr_schedule = [args.train_epochs // 2 + 1]\n",
        "        for e in range(args.train_epochs):\n",
        "\n",
        "            train_loss, train_acc = epoch(\"train\", dataloader=trainloader, net=teacher_net, optimizer=teacher_optim,\n",
        "                                        criterion=criterion, args=args, aug=True,scheduler=scheduler)\n",
        "\n",
        "            test_loss, test_acc = epoch(\"test\", dataloader=testloader, net=teacher_net, optimizer=None,\n",
        "                                        criterion=criterion, args=args, aug=False, scheduler=scheduler)\n",
        "\n",
        "            print(\"Itr: {}\\tEpoch: {}\\tTrain Acc: {}\\tTest Acc: {}\".format(it, e, train_acc, test_acc))\n",
        "\n",
        "            timestamps.append([p.detach().cpu() for p in teacher_net.parameters()])\n",
        "\n",
        "\n",
        "        trajectories.append(timestamps)\n",
        "\n",
        "        if len(trajectories) == args.save_interval:\n",
        "            n = 0\n",
        "            while os.path.exists(os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n))):\n",
        "                n += 1\n",
        "            print(\"Saving {}\".format(os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n))))\n",
        "            torch.save(trajectories, os.path.join(save_dir, \"replay_buffer_{}.pt\".format(n)))\n",
        "            trajectories = []\n"
      ],
      "metadata": {
        "id": "krgJIgjzB2LY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "5N6fzL9R10Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "#Ziyao Guo1, Kai Wang1, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You\n",
        "#https://github.com/GzyAftermath/DATM\n",
        "###############\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import kornia as K\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
        "from networks import MLP, ConvNet, LeNet, AlexNet, VGG11BN, VGG11, ResNet18, ResNet18BN_AP, ResNet18_AP, ResNet18BN\n",
        "\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "def get_dataset(dataset, data_path, batch_size=1, subset=\"imagenette\"):\n",
        "\n",
        "    class_map = None\n",
        "    loader_train_dict = None\n",
        "    class_map_inv = None\n",
        "\n",
        "    if dataset == 'CIFAR10':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 10\n",
        "        mean = [0.4914, 0.4822, 0.4465]\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "        if args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "    elif dataset == 'MNIST':\n",
        "        channel = 1\n",
        "        im_size = (28, 28)\n",
        "        num_classes = 10\n",
        "        mean = [0.1307]\n",
        "        std = [0.3081]\n",
        "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "    elif dataset == 'MHIST':\n",
        "        im_size = (64, 64)\n",
        "        num_classes=2\n",
        "        channel=3\n",
        "        mean = [0,0,0]\n",
        "        std = [0,0,0]\n",
        "        train_dir = './mhist_dataset/augmentation'\n",
        "        #train_dir = './mhist_dataset/train'\n",
        "        test_dir = './mhist_dataset/test'\n",
        "\n",
        "        #mhist_data = datasets.ImageFolder(root=train_dir, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
        "        #mean, std = compute_mean_std(mhist_data)\n",
        "\n",
        "        transform_mhist = transforms.Compose([\n",
        "            transforms.Resize(im_size),\n",
        "            transforms.ToTensor(),\n",
        "            #transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "        dst_train = datasets.ImageFolder(root=train_dir, transform=transform_mhist)\n",
        "        dst_test = datasets.ImageFolder(root=test_dir, transform=transform_mhist)\n",
        "        class_names = [str(c) for c in range(num_classes)]\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "\n",
        "    elif dataset == 'Tiny':\n",
        "        channel = 3\n",
        "        im_size = (64, 64)\n",
        "        num_classes = 200\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        if args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=transform) # no augmentation\n",
        "        dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\", \"images\"), transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "\n",
        "    elif dataset == 'ImageNet':\n",
        "        channel = 3\n",
        "        im_size = (128, 128)\n",
        "        num_classes = 10\n",
        "\n",
        "        config.img_net_classes = config.dict[subset]\n",
        "\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        if args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Resize(im_size),\n",
        "                                        transforms.CenterCrop(im_size)])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.Normalize(mean=mean, std=std),\n",
        "                                            transforms.Resize(im_size),\n",
        "                                            transforms.CenterCrop(im_size)])\n",
        "\n",
        "        dst_train = datasets.ImageNet(data_path, split=\"train\", transform=transform) # no augmentation\n",
        "        dst_train_dict = {c : torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.equal(dst_train.targets, config.img_net_classes[c])))) for c in range(len(config.img_net_classes))}\n",
        "        dst_train = torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.isin(dst_train.targets, config.img_net_classes))))\n",
        "        loader_train_dict = {c : torch.utils.data.DataLoader(dst_train_dict[c], batch_size=batch_size, shuffle=True, num_workers=16) for c in range(len(config.img_net_classes))}\n",
        "        dst_test = datasets.ImageNet(data_path, split=\"val\", transform=transform)\n",
        "        dst_test = torch.utils.data.Subset(dst_test, np.squeeze(np.argwhere(np.isin(dst_test.targets, config.img_net_classes))))\n",
        "        for c in range(len(config.img_net_classes)):\n",
        "            dst_test.dataset.targets[dst_test.dataset.targets == config.img_net_classes[c]] = c\n",
        "            dst_train.dataset.targets[dst_train.dataset.targets == config.img_net_classes[c]] = c\n",
        "        print(dst_test.dataset)\n",
        "        class_map = {x: i for i, x in enumerate(config.img_net_classes)}\n",
        "        class_map_inv = {i: x for i, x in enumerate(config.img_net_classes)}\n",
        "        class_names = None\n",
        "\n",
        "\n",
        "    elif dataset.startswith('CIFAR100'):\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 100\n",
        "        mean = [0.4914, 0.4822, 0.4465]\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "        if args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform)  # no augmentation\n",
        "        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x: x for x in range(num_classes)}\n",
        "\n",
        "    elif dataset == 'ImageNet1K':\n",
        "        channel = 3\n",
        "        im_size = (64, 64)\n",
        "        num_classes = 1000\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        data_transforms = {\n",
        "            'train': transforms.Compose([\n",
        "                # transforms.Resize(im_size),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                # transforms.Resize(im_size),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "        }\n",
        "\n",
        "        # Create datasets and data loaders for training and testing\n",
        "        dst_train = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"train\"), transform=data_transforms['train'])\n",
        "        dst_test = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n",
        "\n",
        "        # dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=data_transforms['train']) # no augmentation\n",
        "        # dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "    else:\n",
        "        exit('unknown dataset: %s'%dataset)\n",
        "\n",
        "    if args.zca:\n",
        "        images = []\n",
        "        labels = []\n",
        "        print(\"Train ZCA\")\n",
        "        for i in tqdm(range(len(dst_train))):\n",
        "            im, lab = dst_train[i]\n",
        "            images.append(im)\n",
        "            labels.append(lab)\n",
        "        images = torch.stack(images, dim=0).to(\"cpu\")\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n",
        "        zca = K.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
        "        zca.fit(images)\n",
        "        zca_images = zca(images).to(\"cpu\")\n",
        "        dst_train = TensorDataset(zca_images, labels)\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "        print(\"Test ZCA\")\n",
        "        for i in tqdm(range(len(dst_test))):\n",
        "            im, lab = dst_test[i]\n",
        "            images.append(im)\n",
        "            labels.append(lab)\n",
        "        images = torch.stack(images, dim=0).to(\"cpu\")\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n",
        "\n",
        "        zca_images = zca(images).to(\"cpu\")\n",
        "        dst_test = TensorDataset(zca_images, labels)\n",
        "\n",
        "        args.zca_trans = zca\n",
        "\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B-3mFxnFF4X",
        "outputId": "55b603b3-0f65-4fe1-dab1-8e979f623df4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-354399d7b5d0>:11: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Expert Trajectories"
      ],
      "metadata": {
        "id": "iVp_Z9D6LOda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VF8IaukhySxA"
      },
      "outputs": [],
      "source": [
        "args = type('', (), {})()\n",
        "args.dataset = 'MNIST'\n",
        "args.model = 'ConvNet'\n",
        "args.num_experts = 4 #training iterations\n",
        "args.lr_teacher = 0.01\n",
        "args.batch_train = 128\n",
        "args.batch_real = 128\n",
        "args.dsa = True\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "args.dsa_param = utils.ParamDiffAug()\n",
        "args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'\n",
        "args.data_path = './data'\n",
        "args.buffer_path = './buffers'\n",
        "args.train_epochs = 100\n",
        "args.mom = 0\n",
        "args.l2 = 0\n",
        "args.save_interval = 1\n",
        "args.rho_max = 2\n",
        "args.rho_min = 2\n",
        "args.alpha = 0.4\n",
        "args.adaptive = True\n",
        "args.zca = 'store_true'\n",
        "args.decay = 'store_true'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlFLauWs5fkL",
        "outputId": "636f9465-e735-4afd-87bf-8bf49ec0bbe0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ZCA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60000/60000 [00:12<00:00, 4833.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ZCA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:01<00:00, 5377.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper-parameters: \n",
            " {'dataset': 'MNIST', 'model': 'ConvNet', 'num_experts': 4, 'lr_teacher': 0.01, 'batch_train': 128, 'batch_real': 128, 'dsa': True, 'device': 'cuda', 'dsa_param': <utils.ParamDiffAug object at 0x7a800edb87f0>, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'data_path': './data', 'buffer_path': './buffers', 'train_epochs': 100, 'mom': 0, 'l2': 0, 'save_interval': 1, 'rho_max': 2, 'rho_min': 2, 'alpha': 0.4, 'adaptive': True, 'zca': 'store_true', 'decay': 'store_true', 'zca_trans': ZCAWhitening()}\n",
            "BUILDING DATASET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/60000 [00:00<?, ?it/s]<ipython-input-11-03ebc642d930>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_all.append(class_map[torch.tensor(sample[1]).item()])\n",
            "100%|██████████| 60000/60000 [00:00<00:00, 60596.33it/s]\n",
            "60000it [00:00, 1451391.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class c = 0: 5923 real images\n",
            "class c = 1: 6742 real images\n",
            "class c = 2: 5958 real images\n",
            "class c = 3: 6131 real images\n",
            "class c = 4: 5842 real images\n",
            "class c = 5: 5421 real images\n",
            "class c = 6: 5918 real images\n",
            "class c = 7: 6265 real images\n",
            "class c = 8: 5851 real images\n",
            "class c = 9: 5949 real images\n",
            "real images channel 0, mean = -0.0000, std = 0.5891\n",
            "DC augmentation parameters: \n",
            " {'crop': 4, 'scale': 0.2, 'rotate': 45, 'noise': 0.001, 'strategy': 'crop_scale_rotate'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Itr: 0\tEpoch: 0\tTrain Acc: 0.75195\tTest Acc: 0.9499\n",
            "Itr: 0\tEpoch: 1\tTrain Acc: 0.8727666666666667\tTest Acc: 0.9625\n",
            "Itr: 0\tEpoch: 2\tTrain Acc: 0.9059833333333334\tTest Acc: 0.9729\n",
            "Itr: 0\tEpoch: 3\tTrain Acc: 0.9170166666666667\tTest Acc: 0.976\n",
            "Itr: 0\tEpoch: 4\tTrain Acc: 0.92355\tTest Acc: 0.9791\n",
            "Itr: 0\tEpoch: 5\tTrain Acc: 0.9304\tTest Acc: 0.9799\n",
            "Itr: 0\tEpoch: 6\tTrain Acc: 0.9366\tTest Acc: 0.9832\n",
            "Itr: 0\tEpoch: 7\tTrain Acc: 0.9404166666666667\tTest Acc: 0.9781\n",
            "Itr: 0\tEpoch: 8\tTrain Acc: 0.94295\tTest Acc: 0.9828\n",
            "Itr: 0\tEpoch: 9\tTrain Acc: 0.94725\tTest Acc: 0.9849\n",
            "Itr: 0\tEpoch: 10\tTrain Acc: 0.9510666666666666\tTest Acc: 0.9865\n",
            "Itr: 0\tEpoch: 11\tTrain Acc: 0.95195\tTest Acc: 0.9877\n",
            "Itr: 0\tEpoch: 12\tTrain Acc: 0.9532333333333334\tTest Acc: 0.9877\n",
            "Itr: 0\tEpoch: 13\tTrain Acc: 0.95665\tTest Acc: 0.9872\n",
            "Itr: 0\tEpoch: 14\tTrain Acc: 0.95825\tTest Acc: 0.9886\n",
            "Itr: 0\tEpoch: 15\tTrain Acc: 0.9612333333333334\tTest Acc: 0.9894\n",
            "Itr: 0\tEpoch: 16\tTrain Acc: 0.96245\tTest Acc: 0.9892\n",
            "Itr: 0\tEpoch: 17\tTrain Acc: 0.9631666666666666\tTest Acc: 0.9897\n",
            "Itr: 0\tEpoch: 18\tTrain Acc: 0.9648166666666667\tTest Acc: 0.9891\n",
            "Itr: 0\tEpoch: 19\tTrain Acc: 0.9664166666666667\tTest Acc: 0.9882\n",
            "Itr: 0\tEpoch: 20\tTrain Acc: 0.9683333333333334\tTest Acc: 0.989\n",
            "Itr: 0\tEpoch: 21\tTrain Acc: 0.9674666666666667\tTest Acc: 0.99\n",
            "Itr: 0\tEpoch: 22\tTrain Acc: 0.9680333333333333\tTest Acc: 0.9908\n",
            "Itr: 0\tEpoch: 23\tTrain Acc: 0.9695166666666667\tTest Acc: 0.9904\n",
            "Itr: 0\tEpoch: 24\tTrain Acc: 0.9693666666666667\tTest Acc: 0.9896\n",
            "Itr: 0\tEpoch: 25\tTrain Acc: 0.9713333333333334\tTest Acc: 0.9902\n",
            "Itr: 0\tEpoch: 26\tTrain Acc: 0.97195\tTest Acc: 0.9916\n",
            "Itr: 0\tEpoch: 27\tTrain Acc: 0.9732166666666666\tTest Acc: 0.9917\n",
            "Itr: 0\tEpoch: 28\tTrain Acc: 0.97235\tTest Acc: 0.9911\n",
            "Itr: 0\tEpoch: 29\tTrain Acc: 0.9728333333333333\tTest Acc: 0.9903\n",
            "Itr: 0\tEpoch: 30\tTrain Acc: 0.97485\tTest Acc: 0.9911\n",
            "Itr: 0\tEpoch: 31\tTrain Acc: 0.9741666666666666\tTest Acc: 0.9913\n",
            "Itr: 0\tEpoch: 32\tTrain Acc: 0.97475\tTest Acc: 0.9912\n",
            "Itr: 0\tEpoch: 33\tTrain Acc: 0.97565\tTest Acc: 0.9911\n",
            "Itr: 0\tEpoch: 34\tTrain Acc: 0.9745166666666667\tTest Acc: 0.9914\n",
            "Itr: 0\tEpoch: 35\tTrain Acc: 0.97635\tTest Acc: 0.9911\n",
            "Itr: 0\tEpoch: 36\tTrain Acc: 0.9764666666666667\tTest Acc: 0.9918\n",
            "Itr: 0\tEpoch: 37\tTrain Acc: 0.9765\tTest Acc: 0.9905\n",
            "Itr: 0\tEpoch: 38\tTrain Acc: 0.9778333333333333\tTest Acc: 0.9921\n",
            "Itr: 0\tEpoch: 39\tTrain Acc: 0.9784333333333334\tTest Acc: 0.992\n",
            "Itr: 0\tEpoch: 40\tTrain Acc: 0.97905\tTest Acc: 0.9918\n",
            "Itr: 0\tEpoch: 41\tTrain Acc: 0.9787833333333333\tTest Acc: 0.9918\n",
            "Itr: 0\tEpoch: 42\tTrain Acc: 0.978\tTest Acc: 0.9904\n",
            "Itr: 0\tEpoch: 43\tTrain Acc: 0.9797833333333333\tTest Acc: 0.9921\n",
            "Itr: 0\tEpoch: 44\tTrain Acc: 0.9794833333333334\tTest Acc: 0.992\n",
            "Itr: 0\tEpoch: 45\tTrain Acc: 0.9795666666666667\tTest Acc: 0.9919\n",
            "Itr: 0\tEpoch: 46\tTrain Acc: 0.9795666666666667\tTest Acc: 0.9911\n",
            "Itr: 0\tEpoch: 47\tTrain Acc: 0.9802333333333333\tTest Acc: 0.9928\n",
            "Itr: 0\tEpoch: 48\tTrain Acc: 0.97945\tTest Acc: 0.9922\n",
            "Itr: 0\tEpoch: 49\tTrain Acc: 0.9809833333333333\tTest Acc: 0.9924\n",
            "Itr: 0\tEpoch: 50\tTrain Acc: 0.9813666666666667\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 51\tTrain Acc: 0.9800666666666666\tTest Acc: 0.9923\n",
            "Itr: 0\tEpoch: 52\tTrain Acc: 0.9813\tTest Acc: 0.9928\n",
            "Itr: 0\tEpoch: 53\tTrain Acc: 0.9821166666666666\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 54\tTrain Acc: 0.9815833333333334\tTest Acc: 0.9922\n",
            "Itr: 0\tEpoch: 55\tTrain Acc: 0.9819833333333333\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 56\tTrain Acc: 0.9819333333333333\tTest Acc: 0.9929\n",
            "Itr: 0\tEpoch: 57\tTrain Acc: 0.9833\tTest Acc: 0.9921\n",
            "Itr: 0\tEpoch: 58\tTrain Acc: 0.98285\tTest Acc: 0.9929\n",
            "Itr: 0\tEpoch: 59\tTrain Acc: 0.9815833333333334\tTest Acc: 0.9926\n",
            "Itr: 0\tEpoch: 60\tTrain Acc: 0.9826333333333334\tTest Acc: 0.9923\n",
            "Itr: 0\tEpoch: 61\tTrain Acc: 0.9841166666666666\tTest Acc: 0.9928\n",
            "Itr: 0\tEpoch: 62\tTrain Acc: 0.9835166666666667\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 63\tTrain Acc: 0.9831\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 64\tTrain Acc: 0.9834833333333334\tTest Acc: 0.9925\n",
            "Itr: 0\tEpoch: 65\tTrain Acc: 0.9828\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 66\tTrain Acc: 0.9840833333333333\tTest Acc: 0.9934\n",
            "Itr: 0\tEpoch: 67\tTrain Acc: 0.98395\tTest Acc: 0.9928\n",
            "Itr: 0\tEpoch: 68\tTrain Acc: 0.9835666666666667\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 69\tTrain Acc: 0.9840833333333333\tTest Acc: 0.9931\n",
            "Itr: 0\tEpoch: 70\tTrain Acc: 0.98395\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 71\tTrain Acc: 0.985\tTest Acc: 0.9929\n",
            "Itr: 0\tEpoch: 72\tTrain Acc: 0.9852333333333333\tTest Acc: 0.9929\n",
            "Itr: 0\tEpoch: 73\tTrain Acc: 0.9846333333333334\tTest Acc: 0.9933\n",
            "Itr: 0\tEpoch: 74\tTrain Acc: 0.9838833333333333\tTest Acc: 0.9926\n",
            "Itr: 0\tEpoch: 75\tTrain Acc: 0.9853833333333334\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 76\tTrain Acc: 0.98515\tTest Acc: 0.9933\n",
            "Itr: 0\tEpoch: 77\tTrain Acc: 0.98495\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 78\tTrain Acc: 0.9845666666666667\tTest Acc: 0.9927\n",
            "Itr: 0\tEpoch: 79\tTrain Acc: 0.98475\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 80\tTrain Acc: 0.9854833333333334\tTest Acc: 0.9926\n",
            "Itr: 0\tEpoch: 81\tTrain Acc: 0.9847333333333333\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 82\tTrain Acc: 0.9847333333333333\tTest Acc: 0.9934\n",
            "Itr: 0\tEpoch: 83\tTrain Acc: 0.9868833333333333\tTest Acc: 0.9931\n",
            "Itr: 0\tEpoch: 84\tTrain Acc: 0.9863833333333333\tTest Acc: 0.9935\n",
            "Itr: 0\tEpoch: 85\tTrain Acc: 0.9853833333333334\tTest Acc: 0.9937\n",
            "Itr: 0\tEpoch: 86\tTrain Acc: 0.9864333333333334\tTest Acc: 0.993\n",
            "Itr: 0\tEpoch: 87\tTrain Acc: 0.9856833333333334\tTest Acc: 0.9931\n",
            "Itr: 0\tEpoch: 88\tTrain Acc: 0.9865666666666667\tTest Acc: 0.9935\n",
            "Itr: 0\tEpoch: 89\tTrain Acc: 0.9864333333333334\tTest Acc: 0.9933\n",
            "Itr: 0\tEpoch: 90\tTrain Acc: 0.9855666666666667\tTest Acc: 0.9935\n",
            "Itr: 0\tEpoch: 91\tTrain Acc: 0.9864\tTest Acc: 0.9933\n",
            "Itr: 0\tEpoch: 92\tTrain Acc: 0.9865166666666667\tTest Acc: 0.9937\n",
            "Itr: 0\tEpoch: 93\tTrain Acc: 0.9857333333333334\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 94\tTrain Acc: 0.9868\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 95\tTrain Acc: 0.9872833333333333\tTest Acc: 0.9934\n",
            "Itr: 0\tEpoch: 96\tTrain Acc: 0.9865166666666667\tTest Acc: 0.994\n",
            "Itr: 0\tEpoch: 97\tTrain Acc: 0.9877666666666667\tTest Acc: 0.9936\n",
            "Itr: 0\tEpoch: 98\tTrain Acc: 0.9887\tTest Acc: 0.9932\n",
            "Itr: 0\tEpoch: 99\tTrain Acc: 0.9866166666666667\tTest Acc: 0.994\n",
            "Saving ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "Itr: 1\tEpoch: 0\tTrain Acc: 0.7665833333333333\tTest Acc: 0.9514\n",
            "Itr: 1\tEpoch: 1\tTrain Acc: 0.8831666666666667\tTest Acc: 0.9641\n",
            "Itr: 1\tEpoch: 2\tTrain Acc: 0.9039\tTest Acc: 0.9748\n",
            "Itr: 1\tEpoch: 3\tTrain Acc: 0.9117333333333333\tTest Acc: 0.9778\n",
            "Itr: 1\tEpoch: 4\tTrain Acc: 0.9284333333333333\tTest Acc: 0.9793\n",
            "Itr: 1\tEpoch: 5\tTrain Acc: 0.9291\tTest Acc: 0.9815\n",
            "Itr: 1\tEpoch: 6\tTrain Acc: 0.93875\tTest Acc: 0.9838\n",
            "Itr: 1\tEpoch: 7\tTrain Acc: 0.9421\tTest Acc: 0.9844\n",
            "Itr: 1\tEpoch: 8\tTrain Acc: 0.9496333333333333\tTest Acc: 0.9856\n",
            "Itr: 1\tEpoch: 9\tTrain Acc: 0.9490833333333333\tTest Acc: 0.9858\n",
            "Itr: 1\tEpoch: 10\tTrain Acc: 0.9521166666666666\tTest Acc: 0.9869\n",
            "Itr: 1\tEpoch: 11\tTrain Acc: 0.9537666666666667\tTest Acc: 0.9877\n",
            "Itr: 1\tEpoch: 12\tTrain Acc: 0.9553333333333334\tTest Acc: 0.9879\n",
            "Itr: 1\tEpoch: 13\tTrain Acc: 0.9595833333333333\tTest Acc: 0.9882\n",
            "Itr: 1\tEpoch: 14\tTrain Acc: 0.95955\tTest Acc: 0.9877\n",
            "Itr: 1\tEpoch: 15\tTrain Acc: 0.96315\tTest Acc: 0.9879\n",
            "Itr: 1\tEpoch: 16\tTrain Acc: 0.9602666666666667\tTest Acc: 0.9896\n",
            "Itr: 1\tEpoch: 17\tTrain Acc: 0.96415\tTest Acc: 0.9891\n",
            "Itr: 1\tEpoch: 18\tTrain Acc: 0.9638333333333333\tTest Acc: 0.9892\n",
            "Itr: 1\tEpoch: 19\tTrain Acc: 0.9654166666666667\tTest Acc: 0.9892\n",
            "Itr: 1\tEpoch: 20\tTrain Acc: 0.96635\tTest Acc: 0.9884\n",
            "Itr: 1\tEpoch: 21\tTrain Acc: 0.9681\tTest Acc: 0.9899\n",
            "Itr: 1\tEpoch: 22\tTrain Acc: 0.9683666666666667\tTest Acc: 0.9896\n",
            "Itr: 1\tEpoch: 23\tTrain Acc: 0.9693\tTest Acc: 0.9897\n",
            "Itr: 1\tEpoch: 24\tTrain Acc: 0.9717666666666667\tTest Acc: 0.9897\n",
            "Itr: 1\tEpoch: 25\tTrain Acc: 0.97125\tTest Acc: 0.9899\n",
            "Itr: 1\tEpoch: 26\tTrain Acc: 0.9724666666666667\tTest Acc: 0.9904\n",
            "Itr: 1\tEpoch: 27\tTrain Acc: 0.9729666666666666\tTest Acc: 0.9905\n",
            "Itr: 1\tEpoch: 28\tTrain Acc: 0.97415\tTest Acc: 0.9892\n",
            "Itr: 1\tEpoch: 29\tTrain Acc: 0.97565\tTest Acc: 0.991\n",
            "Itr: 1\tEpoch: 30\tTrain Acc: 0.9742666666666666\tTest Acc: 0.9914\n",
            "Itr: 1\tEpoch: 31\tTrain Acc: 0.9740833333333333\tTest Acc: 0.9911\n",
            "Itr: 1\tEpoch: 32\tTrain Acc: 0.9763666666666667\tTest Acc: 0.9912\n",
            "Itr: 1\tEpoch: 33\tTrain Acc: 0.9746833333333333\tTest Acc: 0.9916\n",
            "Itr: 1\tEpoch: 34\tTrain Acc: 0.9753\tTest Acc: 0.9911\n",
            "Itr: 1\tEpoch: 35\tTrain Acc: 0.9771333333333333\tTest Acc: 0.9918\n",
            "Itr: 1\tEpoch: 36\tTrain Acc: 0.9780333333333333\tTest Acc: 0.9914\n",
            "Itr: 1\tEpoch: 37\tTrain Acc: 0.978\tTest Acc: 0.9916\n",
            "Itr: 1\tEpoch: 38\tTrain Acc: 0.9774\tTest Acc: 0.9915\n",
            "Itr: 1\tEpoch: 39\tTrain Acc: 0.97735\tTest Acc: 0.992\n",
            "Itr: 1\tEpoch: 40\tTrain Acc: 0.9777833333333333\tTest Acc: 0.9917\n",
            "Itr: 1\tEpoch: 41\tTrain Acc: 0.97675\tTest Acc: 0.9921\n",
            "Itr: 1\tEpoch: 42\tTrain Acc: 0.97965\tTest Acc: 0.9917\n",
            "Itr: 1\tEpoch: 43\tTrain Acc: 0.9794166666666667\tTest Acc: 0.9927\n",
            "Itr: 1\tEpoch: 44\tTrain Acc: 0.9785666666666667\tTest Acc: 0.9919\n",
            "Itr: 1\tEpoch: 45\tTrain Acc: 0.9793833333333334\tTest Acc: 0.9917\n",
            "Itr: 1\tEpoch: 46\tTrain Acc: 0.9809\tTest Acc: 0.9922\n",
            "Itr: 1\tEpoch: 47\tTrain Acc: 0.98065\tTest Acc: 0.9932\n",
            "Itr: 1\tEpoch: 48\tTrain Acc: 0.98005\tTest Acc: 0.9926\n",
            "Itr: 1\tEpoch: 49\tTrain Acc: 0.9811166666666666\tTest Acc: 0.9921\n",
            "Itr: 1\tEpoch: 50\tTrain Acc: 0.9809833333333333\tTest Acc: 0.9927\n",
            "Itr: 1\tEpoch: 51\tTrain Acc: 0.9812666666666666\tTest Acc: 0.993\n",
            "Itr: 1\tEpoch: 52\tTrain Acc: 0.9826833333333334\tTest Acc: 0.9924\n",
            "Itr: 1\tEpoch: 53\tTrain Acc: 0.9824166666666667\tTest Acc: 0.9928\n",
            "Itr: 1\tEpoch: 54\tTrain Acc: 0.9828\tTest Acc: 0.9929\n",
            "Itr: 1\tEpoch: 55\tTrain Acc: 0.9808333333333333\tTest Acc: 0.9929\n",
            "Itr: 1\tEpoch: 56\tTrain Acc: 0.9826\tTest Acc: 0.9922\n",
            "Itr: 1\tEpoch: 57\tTrain Acc: 0.9827\tTest Acc: 0.9926\n",
            "Itr: 1\tEpoch: 58\tTrain Acc: 0.9826833333333334\tTest Acc: 0.9932\n",
            "Itr: 1\tEpoch: 59\tTrain Acc: 0.9818166666666667\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 60\tTrain Acc: 0.9825166666666667\tTest Acc: 0.9929\n",
            "Itr: 1\tEpoch: 61\tTrain Acc: 0.9834333333333334\tTest Acc: 0.9934\n",
            "Itr: 1\tEpoch: 62\tTrain Acc: 0.9830666666666666\tTest Acc: 0.9932\n",
            "Itr: 1\tEpoch: 63\tTrain Acc: 0.9843833333333334\tTest Acc: 0.9933\n",
            "Itr: 1\tEpoch: 64\tTrain Acc: 0.9839\tTest Acc: 0.9931\n",
            "Itr: 1\tEpoch: 65\tTrain Acc: 0.9844166666666667\tTest Acc: 0.9931\n",
            "Itr: 1\tEpoch: 66\tTrain Acc: 0.9830166666666666\tTest Acc: 0.9926\n",
            "Itr: 1\tEpoch: 67\tTrain Acc: 0.9837166666666667\tTest Acc: 0.9924\n",
            "Itr: 1\tEpoch: 68\tTrain Acc: 0.9847666666666667\tTest Acc: 0.9935\n",
            "Itr: 1\tEpoch: 69\tTrain Acc: 0.9850333333333333\tTest Acc: 0.9931\n",
            "Itr: 1\tEpoch: 70\tTrain Acc: 0.9844666666666667\tTest Acc: 0.9938\n",
            "Itr: 1\tEpoch: 71\tTrain Acc: 0.9858166666666667\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 72\tTrain Acc: 0.9850666666666666\tTest Acc: 0.9933\n",
            "Itr: 1\tEpoch: 73\tTrain Acc: 0.9845166666666667\tTest Acc: 0.9934\n",
            "Itr: 1\tEpoch: 74\tTrain Acc: 0.9848666666666667\tTest Acc: 0.9935\n",
            "Itr: 1\tEpoch: 75\tTrain Acc: 0.9847333333333333\tTest Acc: 0.9935\n",
            "Itr: 1\tEpoch: 76\tTrain Acc: 0.9842333333333333\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 77\tTrain Acc: 0.9851333333333333\tTest Acc: 0.9938\n",
            "Itr: 1\tEpoch: 78\tTrain Acc: 0.9849\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 79\tTrain Acc: 0.9846166666666667\tTest Acc: 0.9939\n",
            "Itr: 1\tEpoch: 80\tTrain Acc: 0.9871833333333333\tTest Acc: 0.9936\n",
            "Itr: 1\tEpoch: 81\tTrain Acc: 0.9856166666666667\tTest Acc: 0.994\n",
            "Itr: 1\tEpoch: 82\tTrain Acc: 0.9845833333333334\tTest Acc: 0.9938\n",
            "Itr: 1\tEpoch: 83\tTrain Acc: 0.9852166666666666\tTest Acc: 0.9941\n",
            "Itr: 1\tEpoch: 84\tTrain Acc: 0.9858\tTest Acc: 0.9936\n",
            "Itr: 1\tEpoch: 85\tTrain Acc: 0.9852833333333333\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 86\tTrain Acc: 0.9860333333333333\tTest Acc: 0.9941\n",
            "Itr: 1\tEpoch: 87\tTrain Acc: 0.9864\tTest Acc: 0.9938\n",
            "Itr: 1\tEpoch: 88\tTrain Acc: 0.98585\tTest Acc: 0.994\n",
            "Itr: 1\tEpoch: 89\tTrain Acc: 0.9854166666666667\tTest Acc: 0.9932\n",
            "Itr: 1\tEpoch: 90\tTrain Acc: 0.98645\tTest Acc: 0.9934\n",
            "Itr: 1\tEpoch: 91\tTrain Acc: 0.9866333333333334\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 92\tTrain Acc: 0.9861333333333333\tTest Acc: 0.9939\n",
            "Itr: 1\tEpoch: 93\tTrain Acc: 0.9865333333333334\tTest Acc: 0.9934\n",
            "Itr: 1\tEpoch: 94\tTrain Acc: 0.9861333333333333\tTest Acc: 0.9939\n",
            "Itr: 1\tEpoch: 95\tTrain Acc: 0.9870666666666666\tTest Acc: 0.9933\n",
            "Itr: 1\tEpoch: 96\tTrain Acc: 0.9866166666666667\tTest Acc: 0.9937\n",
            "Itr: 1\tEpoch: 97\tTrain Acc: 0.9869\tTest Acc: 0.9936\n",
            "Itr: 1\tEpoch: 98\tTrain Acc: 0.9876833333333334\tTest Acc: 0.993\n",
            "Itr: 1\tEpoch: 99\tTrain Acc: 0.9876166666666667\tTest Acc: 0.994\n",
            "Saving ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "Itr: 2\tEpoch: 0\tTrain Acc: 0.7633\tTest Acc: 0.9488\n",
            "Itr: 2\tEpoch: 1\tTrain Acc: 0.8896666666666667\tTest Acc: 0.9688\n",
            "Itr: 2\tEpoch: 2\tTrain Acc: 0.9041833333333333\tTest Acc: 0.9723\n",
            "Itr: 2\tEpoch: 3\tTrain Acc: 0.9133\tTest Acc: 0.9785\n",
            "Itr: 2\tEpoch: 4\tTrain Acc: 0.9229833333333334\tTest Acc: 0.9782\n",
            "Itr: 2\tEpoch: 5\tTrain Acc: 0.9318\tTest Acc: 0.9798\n",
            "Itr: 2\tEpoch: 6\tTrain Acc: 0.9398166666666666\tTest Acc: 0.98\n",
            "Itr: 2\tEpoch: 7\tTrain Acc: 0.9396333333333333\tTest Acc: 0.9835\n",
            "Itr: 2\tEpoch: 8\tTrain Acc: 0.9465666666666667\tTest Acc: 0.9846\n",
            "Itr: 2\tEpoch: 9\tTrain Acc: 0.9509833333333333\tTest Acc: 0.9859\n",
            "Itr: 2\tEpoch: 10\tTrain Acc: 0.95115\tTest Acc: 0.9851\n",
            "Itr: 2\tEpoch: 11\tTrain Acc: 0.9527333333333333\tTest Acc: 0.9872\n",
            "Itr: 2\tEpoch: 12\tTrain Acc: 0.9573333333333334\tTest Acc: 0.9869\n",
            "Itr: 2\tEpoch: 13\tTrain Acc: 0.9587666666666667\tTest Acc: 0.9874\n",
            "Itr: 2\tEpoch: 14\tTrain Acc: 0.9601833333333334\tTest Acc: 0.9879\n",
            "Itr: 2\tEpoch: 15\tTrain Acc: 0.9609\tTest Acc: 0.9885\n",
            "Itr: 2\tEpoch: 16\tTrain Acc: 0.96335\tTest Acc: 0.9896\n",
            "Itr: 2\tEpoch: 17\tTrain Acc: 0.9623666666666667\tTest Acc: 0.9894\n",
            "Itr: 2\tEpoch: 18\tTrain Acc: 0.9651166666666666\tTest Acc: 0.9882\n",
            "Itr: 2\tEpoch: 19\tTrain Acc: 0.9671666666666666\tTest Acc: 0.9892\n",
            "Itr: 2\tEpoch: 20\tTrain Acc: 0.9675833333333334\tTest Acc: 0.9897\n",
            "Itr: 2\tEpoch: 21\tTrain Acc: 0.9688166666666667\tTest Acc: 0.99\n",
            "Itr: 2\tEpoch: 22\tTrain Acc: 0.96935\tTest Acc: 0.9904\n",
            "Itr: 2\tEpoch: 23\tTrain Acc: 0.97215\tTest Acc: 0.9903\n",
            "Itr: 2\tEpoch: 24\tTrain Acc: 0.9727333333333333\tTest Acc: 0.9904\n",
            "Itr: 2\tEpoch: 25\tTrain Acc: 0.9707666666666667\tTest Acc: 0.9903\n",
            "Itr: 2\tEpoch: 26\tTrain Acc: 0.9730166666666666\tTest Acc: 0.991\n",
            "Itr: 2\tEpoch: 27\tTrain Acc: 0.9720833333333333\tTest Acc: 0.9902\n",
            "Itr: 2\tEpoch: 28\tTrain Acc: 0.9743666666666667\tTest Acc: 0.9915\n",
            "Itr: 2\tEpoch: 29\tTrain Acc: 0.9730833333333333\tTest Acc: 0.9914\n",
            "Itr: 2\tEpoch: 30\tTrain Acc: 0.9749166666666667\tTest Acc: 0.9915\n",
            "Itr: 2\tEpoch: 31\tTrain Acc: 0.9746\tTest Acc: 0.9916\n",
            "Itr: 2\tEpoch: 32\tTrain Acc: 0.9749\tTest Acc: 0.9914\n",
            "Itr: 2\tEpoch: 33\tTrain Acc: 0.9765\tTest Acc: 0.9909\n",
            "Itr: 2\tEpoch: 34\tTrain Acc: 0.9777333333333333\tTest Acc: 0.9919\n",
            "Itr: 2\tEpoch: 35\tTrain Acc: 0.9760666666666666\tTest Acc: 0.9902\n",
            "Itr: 2\tEpoch: 36\tTrain Acc: 0.9755833333333334\tTest Acc: 0.9906\n",
            "Itr: 2\tEpoch: 37\tTrain Acc: 0.9776166666666667\tTest Acc: 0.9913\n",
            "Itr: 2\tEpoch: 38\tTrain Acc: 0.9789\tTest Acc: 0.9921\n",
            "Itr: 2\tEpoch: 39\tTrain Acc: 0.9787333333333333\tTest Acc: 0.9918\n",
            "Itr: 2\tEpoch: 40\tTrain Acc: 0.9781166666666666\tTest Acc: 0.9915\n",
            "Itr: 2\tEpoch: 41\tTrain Acc: 0.9792666666666666\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 42\tTrain Acc: 0.9798833333333333\tTest Acc: 0.9909\n",
            "Itr: 2\tEpoch: 43\tTrain Acc: 0.9797\tTest Acc: 0.9916\n",
            "Itr: 2\tEpoch: 44\tTrain Acc: 0.9804666666666667\tTest Acc: 0.9921\n",
            "Itr: 2\tEpoch: 45\tTrain Acc: 0.9797333333333333\tTest Acc: 0.9919\n",
            "Itr: 2\tEpoch: 46\tTrain Acc: 0.9807166666666667\tTest Acc: 0.992\n",
            "Itr: 2\tEpoch: 47\tTrain Acc: 0.9803333333333333\tTest Acc: 0.9919\n",
            "Itr: 2\tEpoch: 48\tTrain Acc: 0.9801833333333333\tTest Acc: 0.9916\n",
            "Itr: 2\tEpoch: 49\tTrain Acc: 0.98195\tTest Acc: 0.9914\n",
            "Itr: 2\tEpoch: 50\tTrain Acc: 0.98145\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 51\tTrain Acc: 0.98225\tTest Acc: 0.9924\n",
            "Itr: 2\tEpoch: 52\tTrain Acc: 0.98195\tTest Acc: 0.9918\n",
            "Itr: 2\tEpoch: 53\tTrain Acc: 0.9828666666666667\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 54\tTrain Acc: 0.9823833333333334\tTest Acc: 0.9923\n",
            "Itr: 2\tEpoch: 55\tTrain Acc: 0.9821333333333333\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 56\tTrain Acc: 0.9817666666666667\tTest Acc: 0.992\n",
            "Itr: 2\tEpoch: 57\tTrain Acc: 0.9833833333333334\tTest Acc: 0.9926\n",
            "Itr: 2\tEpoch: 58\tTrain Acc: 0.9823166666666666\tTest Acc: 0.9923\n",
            "Itr: 2\tEpoch: 59\tTrain Acc: 0.9831\tTest Acc: 0.992\n",
            "Itr: 2\tEpoch: 60\tTrain Acc: 0.9823\tTest Acc: 0.9927\n",
            "Itr: 2\tEpoch: 61\tTrain Acc: 0.9821666666666666\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 62\tTrain Acc: 0.9838833333333333\tTest Acc: 0.9929\n",
            "Itr: 2\tEpoch: 63\tTrain Acc: 0.9830166666666666\tTest Acc: 0.9924\n",
            "Itr: 2\tEpoch: 64\tTrain Acc: 0.9840833333333333\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 65\tTrain Acc: 0.9846\tTest Acc: 0.9922\n",
            "Itr: 2\tEpoch: 66\tTrain Acc: 0.9848166666666667\tTest Acc: 0.9929\n",
            "Itr: 2\tEpoch: 67\tTrain Acc: 0.9846333333333334\tTest Acc: 0.9927\n",
            "Itr: 2\tEpoch: 68\tTrain Acc: 0.9841\tTest Acc: 0.9933\n",
            "Itr: 2\tEpoch: 69\tTrain Acc: 0.9850166666666667\tTest Acc: 0.993\n",
            "Itr: 2\tEpoch: 70\tTrain Acc: 0.98415\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 71\tTrain Acc: 0.98545\tTest Acc: 0.9925\n",
            "Itr: 2\tEpoch: 72\tTrain Acc: 0.9850333333333333\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 73\tTrain Acc: 0.9844333333333334\tTest Acc: 0.9928\n",
            "Itr: 2\tEpoch: 74\tTrain Acc: 0.9852166666666666\tTest Acc: 0.9931\n",
            "Itr: 2\tEpoch: 75\tTrain Acc: 0.9853333333333333\tTest Acc: 0.9929\n",
            "Itr: 2\tEpoch: 76\tTrain Acc: 0.9856\tTest Acc: 0.9927\n",
            "Itr: 2\tEpoch: 77\tTrain Acc: 0.9855166666666667\tTest Acc: 0.9936\n",
            "Itr: 2\tEpoch: 78\tTrain Acc: 0.986\tTest Acc: 0.9935\n",
            "Itr: 2\tEpoch: 79\tTrain Acc: 0.98605\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 80\tTrain Acc: 0.9849\tTest Acc: 0.9936\n",
            "Itr: 2\tEpoch: 81\tTrain Acc: 0.98635\tTest Acc: 0.9929\n",
            "Itr: 2\tEpoch: 82\tTrain Acc: 0.98645\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 83\tTrain Acc: 0.9869666666666667\tTest Acc: 0.9937\n",
            "Itr: 2\tEpoch: 84\tTrain Acc: 0.9856166666666667\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 85\tTrain Acc: 0.9868333333333333\tTest Acc: 0.9931\n",
            "Itr: 2\tEpoch: 86\tTrain Acc: 0.9867166666666667\tTest Acc: 0.9936\n",
            "Itr: 2\tEpoch: 87\tTrain Acc: 0.98545\tTest Acc: 0.993\n",
            "Itr: 2\tEpoch: 88\tTrain Acc: 0.9865666666666667\tTest Acc: 0.9935\n",
            "Itr: 2\tEpoch: 89\tTrain Acc: 0.9875166666666667\tTest Acc: 0.9926\n",
            "Itr: 2\tEpoch: 90\tTrain Acc: 0.98555\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 91\tTrain Acc: 0.98675\tTest Acc: 0.9934\n",
            "Itr: 2\tEpoch: 92\tTrain Acc: 0.9869\tTest Acc: 0.9934\n",
            "Itr: 2\tEpoch: 93\tTrain Acc: 0.9867833333333333\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 94\tTrain Acc: 0.98755\tTest Acc: 0.9935\n",
            "Itr: 2\tEpoch: 95\tTrain Acc: 0.9879166666666667\tTest Acc: 0.9941\n",
            "Itr: 2\tEpoch: 96\tTrain Acc: 0.98845\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 97\tTrain Acc: 0.98795\tTest Acc: 0.9932\n",
            "Itr: 2\tEpoch: 98\tTrain Acc: 0.98705\tTest Acc: 0.9935\n",
            "Itr: 2\tEpoch: 99\tTrain Acc: 0.9873166666666666\tTest Acc: 0.9932\n",
            "Saving ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "Itr: 3\tEpoch: 0\tTrain Acc: 0.7657\tTest Acc: 0.9489\n",
            "Itr: 3\tEpoch: 1\tTrain Acc: 0.8790333333333333\tTest Acc: 0.9641\n",
            "Itr: 3\tEpoch: 2\tTrain Acc: 0.9015\tTest Acc: 0.9737\n",
            "Itr: 3\tEpoch: 3\tTrain Acc: 0.9132833333333333\tTest Acc: 0.9761\n",
            "Itr: 3\tEpoch: 4\tTrain Acc: 0.9274333333333333\tTest Acc: 0.9809\n",
            "Itr: 3\tEpoch: 5\tTrain Acc: 0.9279333333333334\tTest Acc: 0.9796\n",
            "Itr: 3\tEpoch: 6\tTrain Acc: 0.94065\tTest Acc: 0.9814\n",
            "Itr: 3\tEpoch: 7\tTrain Acc: 0.94315\tTest Acc: 0.9853\n",
            "Itr: 3\tEpoch: 8\tTrain Acc: 0.9441333333333334\tTest Acc: 0.9831\n",
            "Itr: 3\tEpoch: 9\tTrain Acc: 0.9492333333333334\tTest Acc: 0.9854\n",
            "Itr: 3\tEpoch: 10\tTrain Acc: 0.9530166666666666\tTest Acc: 0.9863\n",
            "Itr: 3\tEpoch: 11\tTrain Acc: 0.9578666666666666\tTest Acc: 0.9871\n",
            "Itr: 3\tEpoch: 12\tTrain Acc: 0.9570833333333333\tTest Acc: 0.9877\n",
            "Itr: 3\tEpoch: 13\tTrain Acc: 0.9576166666666667\tTest Acc: 0.9862\n",
            "Itr: 3\tEpoch: 14\tTrain Acc: 0.9609833333333333\tTest Acc: 0.9884\n",
            "Itr: 3\tEpoch: 15\tTrain Acc: 0.96155\tTest Acc: 0.9874\n",
            "Itr: 3\tEpoch: 16\tTrain Acc: 0.9632833333333334\tTest Acc: 0.9878\n",
            "Itr: 3\tEpoch: 17\tTrain Acc: 0.9639833333333333\tTest Acc: 0.9887\n",
            "Itr: 3\tEpoch: 18\tTrain Acc: 0.9669333333333333\tTest Acc: 0.9891\n",
            "Itr: 3\tEpoch: 19\tTrain Acc: 0.9673833333333334\tTest Acc: 0.9898\n",
            "Itr: 3\tEpoch: 20\tTrain Acc: 0.96745\tTest Acc: 0.9896\n",
            "Itr: 3\tEpoch: 21\tTrain Acc: 0.9681\tTest Acc: 0.99\n",
            "Itr: 3\tEpoch: 22\tTrain Acc: 0.96905\tTest Acc: 0.9889\n",
            "Itr: 3\tEpoch: 23\tTrain Acc: 0.9704833333333334\tTest Acc: 0.9896\n",
            "Itr: 3\tEpoch: 24\tTrain Acc: 0.972\tTest Acc: 0.9901\n",
            "Itr: 3\tEpoch: 25\tTrain Acc: 0.9704333333333334\tTest Acc: 0.9902\n",
            "Itr: 3\tEpoch: 26\tTrain Acc: 0.9713\tTest Acc: 0.9912\n",
            "Itr: 3\tEpoch: 27\tTrain Acc: 0.9735333333333334\tTest Acc: 0.9909\n",
            "Itr: 3\tEpoch: 28\tTrain Acc: 0.9734166666666667\tTest Acc: 0.9908\n",
            "Itr: 3\tEpoch: 29\tTrain Acc: 0.9755\tTest Acc: 0.9918\n",
            "Itr: 3\tEpoch: 30\tTrain Acc: 0.9763333333333334\tTest Acc: 0.9904\n",
            "Itr: 3\tEpoch: 31\tTrain Acc: 0.9767833333333333\tTest Acc: 0.991\n",
            "Itr: 3\tEpoch: 32\tTrain Acc: 0.9752666666666666\tTest Acc: 0.9914\n",
            "Itr: 3\tEpoch: 33\tTrain Acc: 0.9775833333333334\tTest Acc: 0.9921\n",
            "Itr: 3\tEpoch: 34\tTrain Acc: 0.9761833333333333\tTest Acc: 0.9916\n",
            "Itr: 3\tEpoch: 35\tTrain Acc: 0.9781666666666666\tTest Acc: 0.9913\n",
            "Itr: 3\tEpoch: 36\tTrain Acc: 0.9774333333333334\tTest Acc: 0.9922\n",
            "Itr: 3\tEpoch: 37\tTrain Acc: 0.9775\tTest Acc: 0.9916\n",
            "Itr: 3\tEpoch: 38\tTrain Acc: 0.9782\tTest Acc: 0.9923\n",
            "Itr: 3\tEpoch: 39\tTrain Acc: 0.9787666666666667\tTest Acc: 0.9917\n",
            "Itr: 3\tEpoch: 40\tTrain Acc: 0.9781666666666666\tTest Acc: 0.9923\n",
            "Itr: 3\tEpoch: 41\tTrain Acc: 0.9785333333333334\tTest Acc: 0.9922\n",
            "Itr: 3\tEpoch: 42\tTrain Acc: 0.9794333333333334\tTest Acc: 0.9922\n",
            "Itr: 3\tEpoch: 43\tTrain Acc: 0.9799833333333333\tTest Acc: 0.9923\n",
            "Itr: 3\tEpoch: 44\tTrain Acc: 0.9791333333333333\tTest Acc: 0.9918\n",
            "Itr: 3\tEpoch: 45\tTrain Acc: 0.9798333333333333\tTest Acc: 0.9927\n",
            "Itr: 3\tEpoch: 46\tTrain Acc: 0.9801\tTest Acc: 0.9928\n",
            "Itr: 3\tEpoch: 47\tTrain Acc: 0.9810166666666666\tTest Acc: 0.9921\n",
            "Itr: 3\tEpoch: 48\tTrain Acc: 0.9805\tTest Acc: 0.9923\n",
            "Itr: 3\tEpoch: 49\tTrain Acc: 0.9813666666666667\tTest Acc: 0.9928\n",
            "Itr: 3\tEpoch: 50\tTrain Acc: 0.98095\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 51\tTrain Acc: 0.9809833333333333\tTest Acc: 0.9926\n",
            "Itr: 3\tEpoch: 52\tTrain Acc: 0.9817166666666667\tTest Acc: 0.9928\n",
            "Itr: 3\tEpoch: 53\tTrain Acc: 0.9827333333333333\tTest Acc: 0.9931\n",
            "Itr: 3\tEpoch: 54\tTrain Acc: 0.9819333333333333\tTest Acc: 0.9935\n",
            "Itr: 3\tEpoch: 55\tTrain Acc: 0.9816666666666667\tTest Acc: 0.9933\n",
            "Itr: 3\tEpoch: 56\tTrain Acc: 0.98255\tTest Acc: 0.9923\n",
            "Itr: 3\tEpoch: 57\tTrain Acc: 0.98255\tTest Acc: 0.9933\n",
            "Itr: 3\tEpoch: 58\tTrain Acc: 0.9840166666666667\tTest Acc: 0.9927\n",
            "Itr: 3\tEpoch: 59\tTrain Acc: 0.98305\tTest Acc: 0.9927\n",
            "Itr: 3\tEpoch: 60\tTrain Acc: 0.9833833333333334\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 61\tTrain Acc: 0.9837333333333333\tTest Acc: 0.9932\n",
            "Itr: 3\tEpoch: 62\tTrain Acc: 0.9833833333333334\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 63\tTrain Acc: 0.9850333333333333\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 64\tTrain Acc: 0.9840166666666667\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 65\tTrain Acc: 0.9846333333333334\tTest Acc: 0.9928\n",
            "Itr: 3\tEpoch: 66\tTrain Acc: 0.984\tTest Acc: 0.9931\n",
            "Itr: 3\tEpoch: 67\tTrain Acc: 0.9839166666666667\tTest Acc: 0.9931\n",
            "Itr: 3\tEpoch: 68\tTrain Acc: 0.9831833333333333\tTest Acc: 0.9929\n",
            "Itr: 3\tEpoch: 69\tTrain Acc: 0.9850833333333333\tTest Acc: 0.9936\n",
            "Itr: 3\tEpoch: 70\tTrain Acc: 0.984\tTest Acc: 0.9932\n",
            "Itr: 3\tEpoch: 71\tTrain Acc: 0.9838\tTest Acc: 0.9938\n",
            "Itr: 3\tEpoch: 72\tTrain Acc: 0.9843166666666666\tTest Acc: 0.9936\n",
            "Itr: 3\tEpoch: 73\tTrain Acc: 0.9840166666666667\tTest Acc: 0.9931\n",
            "Itr: 3\tEpoch: 74\tTrain Acc: 0.98515\tTest Acc: 0.9932\n",
            "Itr: 3\tEpoch: 75\tTrain Acc: 0.9865\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 76\tTrain Acc: 0.9858\tTest Acc: 0.9932\n",
            "Itr: 3\tEpoch: 77\tTrain Acc: 0.9855\tTest Acc: 0.9929\n",
            "Itr: 3\tEpoch: 78\tTrain Acc: 0.9854333333333334\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 79\tTrain Acc: 0.9850833333333333\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 80\tTrain Acc: 0.9845\tTest Acc: 0.9936\n",
            "Itr: 3\tEpoch: 81\tTrain Acc: 0.98625\tTest Acc: 0.993\n",
            "Itr: 3\tEpoch: 82\tTrain Acc: 0.9856666666666667\tTest Acc: 0.9931\n",
            "Itr: 3\tEpoch: 83\tTrain Acc: 0.9859833333333333\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 84\tTrain Acc: 0.9860333333333333\tTest Acc: 0.9937\n",
            "Itr: 3\tEpoch: 85\tTrain Acc: 0.98555\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 86\tTrain Acc: 0.98625\tTest Acc: 0.9935\n",
            "Itr: 3\tEpoch: 87\tTrain Acc: 0.9864166666666667\tTest Acc: 0.9935\n",
            "Itr: 3\tEpoch: 88\tTrain Acc: 0.9861333333333333\tTest Acc: 0.9936\n",
            "Itr: 3\tEpoch: 89\tTrain Acc: 0.9863\tTest Acc: 0.9938\n",
            "Itr: 3\tEpoch: 90\tTrain Acc: 0.9864166666666667\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 91\tTrain Acc: 0.9872166666666666\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 92\tTrain Acc: 0.98675\tTest Acc: 0.9937\n",
            "Itr: 3\tEpoch: 93\tTrain Acc: 0.98645\tTest Acc: 0.9935\n",
            "Itr: 3\tEpoch: 94\tTrain Acc: 0.9864833333333334\tTest Acc: 0.9935\n",
            "Itr: 3\tEpoch: 95\tTrain Acc: 0.9866333333333334\tTest Acc: 0.9942\n",
            "Itr: 3\tEpoch: 96\tTrain Acc: 0.9873\tTest Acc: 0.9937\n",
            "Itr: 3\tEpoch: 97\tTrain Acc: 0.9874\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 98\tTrain Acc: 0.98735\tTest Acc: 0.9934\n",
            "Itr: 3\tEpoch: 99\tTrain Acc: 0.9877333333333334\tTest Acc: 0.9938\n",
            "Saving ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More packages\n"
      ],
      "metadata": {
        "id": "q8akzbQFmPS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###############\n",
        "#Ziyao Guo1, Kai Wang1, George Cazenavette, Hui Li, Kaipeng Zhang, Yang You\n",
        "#https://github.com/GzyAftermath/DATM\n",
        "###############\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import kornia as K\n",
        "import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from scipy.ndimage.interpolation import rotate as scipyrotate\n",
        "from networks import ConvNet\n",
        "import math\n",
        "from torch.utils.data import Subset\n",
        "def get_dataset(dataset, data_path, batch_size=1, subset=\"imagenette\", args=None, baseline=False):\n",
        "\n",
        "    class_map = None\n",
        "    loader_train_dict = None\n",
        "    class_map_inv = None\n",
        "\n",
        "    if dataset == 'CIFAR10':\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 10\n",
        "        mean = [0.4914, 0.4822, 0.4465]\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "        if baseline:\n",
        "            transform =transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=mean, std=std),\n",
        "            ])\n",
        "        elif args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
        "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "\n",
        "    elif dataset == 'Tiny':\n",
        "        channel = 3\n",
        "        im_size = (64, 64)\n",
        "        num_classes = 200\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        if baseline:\n",
        "            transform =transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomCrop(64, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=mean, std=std),\n",
        "            ])\n",
        "        elif args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=transform) # no augmentation\n",
        "        dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\", \"images\"), transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "\n",
        "    elif dataset == 'ImageNet':\n",
        "        channel = 3\n",
        "        im_size = (128, 128)\n",
        "        num_classes = 10\n",
        "\n",
        "        config.img_net_classes = config.dict[subset]\n",
        "\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "        if args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Resize(im_size),\n",
        "                                        transforms.CenterCrop(im_size)])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.Normalize(mean=mean, std=std),\n",
        "                                            transforms.Resize(im_size),\n",
        "                                            transforms.CenterCrop(im_size)])\n",
        "\n",
        "        dst_train = datasets.ImageNet(data_path, split=\"train\", transform=transform) # no augmentation\n",
        "        dst_train_dict = {c : torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.equal(dst_train.targets, config.img_net_classes[c])))) for c in range(len(config.img_net_classes))}\n",
        "        dst_train = torch.utils.data.Subset(dst_train, np.squeeze(np.argwhere(np.isin(dst_train.targets, config.img_net_classes))))\n",
        "        loader_train_dict = {c : torch.utils.data.DataLoader(dst_train_dict[c], batch_size=batch_size, shuffle=True, num_workers=16) for c in range(len(config.img_net_classes))}\n",
        "        dst_test = datasets.ImageNet(data_path, split=\"val\", transform=transform)\n",
        "        dst_test = torch.utils.data.Subset(dst_test, np.squeeze(np.argwhere(np.isin(dst_test.targets, config.img_net_classes))))\n",
        "        for c in range(len(config.img_net_classes)):\n",
        "            dst_test.dataset.targets[dst_test.dataset.targets == config.img_net_classes[c]] = c\n",
        "            dst_train.dataset.targets[dst_train.dataset.targets == config.img_net_classes[c]] = c\n",
        "        print(dst_test.dataset)\n",
        "        class_map = {x: i for i, x in enumerate(config.img_net_classes)}\n",
        "        class_map_inv = {i: x for i, x in enumerate(config.img_net_classes)}\n",
        "        class_names = None\n",
        "\n",
        "\n",
        "    elif dataset.startswith('CIFAR100'):\n",
        "        channel = 3\n",
        "        im_size = (32, 32)\n",
        "        num_classes = 100\n",
        "        mean = [0.4914, 0.4822, 0.4465]\n",
        "        std = [0.2023, 0.1994, 0.2010]\n",
        "\n",
        "        if baseline:\n",
        "            transform =transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=mean, std=std),\n",
        "            ])\n",
        "        elif args.zca:\n",
        "            transform = transforms.Compose([transforms.ToTensor()])\n",
        "        else:\n",
        "            transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform)  # no augmentation\n",
        "        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x: x for x in range(num_classes)}\n",
        "\n",
        "    elif dataset == 'ImageNet1K':\n",
        "        channel = 3\n",
        "        im_size = (64, 64)\n",
        "        num_classes = 1000\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "\n",
        "        data_transforms = {\n",
        "            'train': transforms.Compose([\n",
        "                # transforms.Resize(im_size),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                # transforms.Resize(im_size),\n",
        "                # transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "        }\n",
        "\n",
        "        # Create datasets and data loaders for training and testing\n",
        "        dst_train = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"train\"), transform=data_transforms['train'])\n",
        "        dst_test = ResizedImageNetDataset(root_dir=os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n",
        "\n",
        "        # dst_train = datasets.ImageFolder(os.path.join(data_path, \"train\"), transform=data_transforms['train']) # no augmentation\n",
        "        # dst_test = datasets.ImageFolder(os.path.join(data_path, \"val\"), transform=data_transforms['val'])\n",
        "        class_names = dst_train.classes\n",
        "        class_map = {x:x for x in range(num_classes)}\n",
        "\n",
        "    else:\n",
        "        exit('unknown dataset: %s'%dataset)\n",
        "\n",
        "\n",
        "\n",
        "    if args.zca:\n",
        "        images = []\n",
        "        labels = []\n",
        "        print(\"Train ZCA\")\n",
        "        for i in tqdm.tqdm(range(len(dst_train))):\n",
        "            im, lab = dst_train[i]\n",
        "            images.append(im)\n",
        "            labels.append(lab)\n",
        "        images = torch.stack(images, dim=0).to(args.device)\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n",
        "        zca = K.enhance.ZCAWhitening(eps=0.1, compute_inv=True)\n",
        "        zca.fit(images)\n",
        "        zca_images = zca(images).to(\"cpu\")\n",
        "        dst_train = TensorDataset(zca_images, labels)\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "        print(\"Test ZCA\")\n",
        "        for i in tqdm.tqdm(range(len(dst_test))):\n",
        "            im, lab = dst_test[i]\n",
        "            images.append(im)\n",
        "            labels.append(lab)\n",
        "        images = torch.stack(images, dim=0).to(args.device)\n",
        "        labels = torch.tensor(labels, dtype=torch.long, device=\"cpu\")\n",
        "\n",
        "        zca_images = zca(images).to(\"cpu\")\n",
        "        dst_test = TensorDataset(zca_images, labels)\n",
        "\n",
        "        print(type(zca))\n",
        "\n",
        "\n",
        "        args.zca_trans = zca\n",
        "\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv\n",
        "\n",
        "\n",
        "\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
        "        self.images = images.detach().float()\n",
        "        self.labels = labels.detach()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.images[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.images.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "def get_default_convnet_setting():\n",
        "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
        "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
        "\n",
        "\n",
        "\n",
        "def get_network(model, channel, num_classes, im_size=(32, 32), dist=True):\n",
        "    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n",
        "    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
        "\n",
        "    if model == 'MLP':\n",
        "        net = MLP(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'MLP_Tiny':\n",
        "        net = MLP(channel=channel, num_classes=num_classes,res=64)\n",
        "    elif model == 'ConvNet':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'LeNet':\n",
        "        net = LeNet(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'LeNet_Tiny':\n",
        "        net = LeNet(channel=channel, num_classes=num_classes, res=64)\n",
        "    elif model == 'AlexNet':\n",
        "        net = AlexNet(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'AlexNet_Tiny':\n",
        "        net = AlexNet(channel=channel, num_classes=num_classes, res=64)\n",
        "    elif model == 'VGG11':\n",
        "        net = VGG11( channel=channel, num_classes=num_classes)\n",
        "    elif model == 'VGG11_Tiny':\n",
        "        net = VGG11_Tiny( channel=channel, num_classes=num_classes)\n",
        "    elif model == 'VGG11BN':\n",
        "        net = VGG11BN(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18':\n",
        "        net = ResNet18(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18BN_AP':\n",
        "        net = ResNet18BN_AP(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18_AP':\n",
        "        net = ResNet18_AP(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18BN':\n",
        "        net = ResNet18BN(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18_Tiny':\n",
        "        net = ResNet18_Tiny(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ResNet18BN_Tiny':\n",
        "        net = ResNet18BN_Tiny(channel=channel, num_classes=num_classes)\n",
        "    elif model == 'ConvNetD1':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD2':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD3':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD4':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD4BN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "    elif model == 'ConvNetD5':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=5, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD6':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=6, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD7':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=7, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "    elif model == 'ConvNetD8':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=8, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
        "\n",
        "\n",
        "    elif model == 'ConvNetW32':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetW64':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetW128':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetW256':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetW512':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=512, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetW1024':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=1024, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling)\n",
        "\n",
        "    elif model == \"ConvNetKIP\":\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=1024, net_depth=net_depth, net_act=net_act,\n",
        "                      net_norm=\"none\", net_pooling=net_pooling)\n",
        "\n",
        "    elif model == 'ConvNetAS':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetAR':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetAL':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling)\n",
        "\n",
        "    elif model == 'ConvNetNN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetBN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetLN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetIN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling)\n",
        "    elif model == 'ConvNetGN':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling)\n",
        "\n",
        "    elif model == 'ConvNetNP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none')\n",
        "    elif model == 'ConvNetMP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling')\n",
        "    elif model == 'ConvNetAP':\n",
        "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling')\n",
        "\n",
        "\n",
        "    else:\n",
        "        net = None\n",
        "        exit('DC error: unknown model')\n",
        "\n",
        "    if dist:\n",
        "        gpu_num = torch.cuda.device_count()\n",
        "        if gpu_num>0:\n",
        "            device = 'cuda'\n",
        "            if gpu_num>1:\n",
        "                net = nn.DataParallel(net)\n",
        "        else:\n",
        "            device = 'cpu'\n",
        "        net = net.to(device)\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "def get_time():\n",
        "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
        "\n",
        "\n",
        "def epoch(mode, dataloader, net, optimizer, criterion, args, aug, texture=False, If_Float = False):\n",
        "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
        "    if args.parall_eva==False:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = args.device\n",
        "\n",
        "    if args.dataset == \"ImageNet\":\n",
        "        class_map = {x: i for i, x in enumerate(config.img_net_classes)}\n",
        "\n",
        "    if mode == 'train':\n",
        "        net.train()\n",
        "    else:\n",
        "        net.eval()\n",
        "    net = net.to(device)\n",
        "    for i_batch, datum in enumerate(dataloader):\n",
        "        img = datum[0].float().to(device)\n",
        "        if If_Float:\n",
        "            lab = datum[1].float().to(device)\n",
        "        else:\n",
        "            lab = datum[1].long().to(device)\n",
        "        if aug:\n",
        "            if args.dsa:\n",
        "                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n",
        "            else:\n",
        "                img = augment(img, args.dc_aug_param, device=device)\n",
        "        img = img.to(device)\n",
        "        if args.dataset == \"ImageNet\" and mode != \"train\":\n",
        "            lab = torch.tensor([class_map[x.item()] for x in lab]).to(device)\n",
        "\n",
        "        n_b = lab.shape[0]\n",
        "\n",
        "        output = net(img)\n",
        "        loss = criterion(output, lab)\n",
        "\n",
        "\n",
        "        if If_Float:\n",
        "            acc = 1.\n",
        "        else:\n",
        "            acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
        "\n",
        "        loss_avg += loss.item()*n_b\n",
        "        acc_avg += acc\n",
        "        num_exp += n_b\n",
        "\n",
        "        if mode == 'train':\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    loss_avg /= num_exp\n",
        "    acc_avg /= num_exp\n",
        "\n",
        "    return loss_avg, acc_avg\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args, return_loss=False, texture=False, train_criterion=None, Preciser_Scheduler=False, type=1):\n",
        "    if args.parall_eva==False:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = args.device\n",
        "    net = net.to(device)\n",
        "    images_train.to(device)\n",
        "    labels_train.to(device)\n",
        "    lr = float(args.lr_net)\n",
        "    Epoch = int(args.epoch_eval_train)\n",
        "\n",
        "    if Preciser_Scheduler:\n",
        "        LR_begin=0.0000000001\n",
        "        LR_End = float(args.lr_net)\n",
        "        if type==0:\n",
        "            t=0\n",
        "        else:\n",
        "            t=500\n",
        "        T=Epoch\n",
        "        lambda1 = lambda epoch: ((LR_End-LR_begin)*epoch / t) if epoch < t else  LR_End * (1+math.cos(math.pi*(epoch - t)/(T-t)))/2.\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=LR_End, weight_decay=0.0005)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "    else:\n",
        "        lr_schedule = [Epoch//2+1]\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    '''test'''\n",
        "    test_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    If_Float = True\n",
        "    if train_criterion == None:\n",
        "        train_criterion = nn.CrossEntropyLoss().to(device)\n",
        "        If_Float = False\n",
        "\n",
        "    dst_train = TensorDataset(images_train, labels_train)\n",
        "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
        "\n",
        "    start = time.time()\n",
        "    acc_train_list = []\n",
        "    loss_train_list = []\n",
        "\n",
        "    for ep in tqdm.tqdm(range(Epoch+1)):\n",
        "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, train_criterion, args, aug=True, texture=texture, If_Float=If_Float)\n",
        "        acc_train_list.append(acc_train)\n",
        "        loss_train_list.append(loss_train)\n",
        "        if ep == Epoch:\n",
        "            with torch.no_grad():\n",
        "                loss_test, acc_test = epoch('test', testloader, net, optimizer, test_criterion, args, aug=False, If_Float = False)\n",
        "        if Preciser_Scheduler:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            if ep in lr_schedule:\n",
        "                lr *= 0.1\n",
        "                optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "\n",
        "    time_train = time.time() - start\n",
        "\n",
        "    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n",
        "\n",
        "    if return_loss:\n",
        "        return net, acc_train_list, acc_test, loss_train_list, loss_test\n",
        "    else:\n",
        "        return net, acc_train_list, acc_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_baseline(it_eval, net, trainloader, testloader, args, return_loss=False, texture=False, train_criterion=None, Preciser_Scheduler=False, type=1):\n",
        "    if args.parall_eva==False:\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = args.device\n",
        "    net = net.to(device)\n",
        "    lr = float(args.lr_net)\n",
        "    Epoch = int(args.epoch_eval_train)\n",
        "\n",
        "    if Preciser_Scheduler:\n",
        "        LR_begin=0.0000000001\n",
        "        LR_End = float(args.lr_net)\n",
        "        if type==0:\n",
        "            t=0\n",
        "        else:\n",
        "            t=500\n",
        "        T=Epoch\n",
        "        lambda1 = lambda epoch: ((LR_End-LR_begin)*epoch / t) if epoch < t else  LR_End * (1+math.cos(math.pi*(epoch - t)/(T-t)))/2.\n",
        "        optimizer = torch.optim.Adam(net.parameters(), lr=LR_End, weight_decay=0.0005)\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "    else:\n",
        "        lr_schedule = [Epoch//2+1]\n",
        "        optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "    '''test'''\n",
        "    test_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    If_Float = True\n",
        "    if train_criterion == None:\n",
        "        train_criterion = nn.CrossEntropyLoss().to(device)\n",
        "        If_Float = False\n",
        "\n",
        "    start = time.time()\n",
        "    acc_train_list = []\n",
        "    loss_train_list = []\n",
        "\n",
        "    for ep in tqdm.tqdm(range(Epoch+1)):\n",
        "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, train_criterion, args, aug=True, texture=texture, If_Float=If_Float)\n",
        "        acc_train_list.append(acc_train)\n",
        "        loss_train_list.append(loss_train)\n",
        "        if ep == Epoch:\n",
        "            with torch.no_grad():\n",
        "                loss_test, acc_test = epoch('test', testloader, net, optimizer, test_criterion, args, aug=False, If_Float = False)\n",
        "        if Preciser_Scheduler:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            if ep in lr_schedule:\n",
        "                lr *= 0.1\n",
        "                optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "    time_train = time.time() - start\n",
        "    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n",
        "    if return_loss:\n",
        "        return net, acc_train_list, acc_test, loss_train_list, loss_test\n",
        "    else:\n",
        "        return net, acc_train_list, acc_test\n",
        "\n",
        "\n",
        "def augment(images, dc_aug_param, device):\n",
        "    # This can be sped up in the future.\n",
        "\n",
        "    if dc_aug_param != None and dc_aug_param['strategy'] != 'none':\n",
        "        scale = dc_aug_param['scale']\n",
        "        crop = dc_aug_param['crop']\n",
        "        rotate = dc_aug_param['rotate']\n",
        "        noise = dc_aug_param['noise']\n",
        "        strategy = dc_aug_param['strategy']\n",
        "\n",
        "        shape = images.shape\n",
        "        mean = []\n",
        "        for c in range(shape[1]):\n",
        "            mean.append(float(torch.mean(images[:,c])))\n",
        "\n",
        "        def cropfun(i):\n",
        "            im_ = torch.zeros(shape[1],shape[2]+crop*2,shape[3]+crop*2, dtype=torch.float, device=device)\n",
        "            for c in range(shape[1]):\n",
        "                im_[c] = mean[c]\n",
        "            im_[:, crop:crop+shape[2], crop:crop+shape[3]] = images[i]\n",
        "            r, c = np.random.permutation(crop*2)[0], np.random.permutation(crop*2)[0]\n",
        "            images[i] = im_[:, r:r+shape[2], c:c+shape[3]]\n",
        "\n",
        "        def scalefun(i):\n",
        "            h = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
        "            w = int((np.random.uniform(1 - scale, 1 + scale)) * shape[2])\n",
        "            tmp = F.interpolate(images[i:i + 1], [h, w], )[0]\n",
        "            mhw = max(h, w, shape[2], shape[3])\n",
        "            im_ = torch.zeros(shape[1], mhw, mhw, dtype=torch.float, device=device)\n",
        "            r = int((mhw - h) / 2)\n",
        "            c = int((mhw - w) / 2)\n",
        "            im_[:, r:r + h, c:c + w] = tmp\n",
        "            r = int((mhw - shape[2]) / 2)\n",
        "            c = int((mhw - shape[3]) / 2)\n",
        "            images[i] = im_[:, r:r + shape[2], c:c + shape[3]]\n",
        "\n",
        "        def rotatefun(i):\n",
        "            im_ = scipyrotate(images[i].cpu().data.numpy(), angle=np.random.randint(-rotate, rotate), axes=(-2, -1), cval=np.mean(mean))\n",
        "            r = int((im_.shape[-2] - shape[-2]) / 2)\n",
        "            c = int((im_.shape[-1] - shape[-1]) / 2)\n",
        "            images[i] = torch.tensor(im_[:, r:r + shape[-2], c:c + shape[-1]], dtype=torch.float, device=device)\n",
        "\n",
        "        def noisefun(i):\n",
        "            images[i] = images[i] + noise * torch.randn(shape[1:], dtype=torch.float, device=device)\n",
        "\n",
        "\n",
        "        augs = strategy.split('_')\n",
        "\n",
        "        for i in range(shape[0]):\n",
        "            choice = np.random.permutation(augs)[0] # randomly implement one augmentation\n",
        "            if choice == 'crop':\n",
        "                cropfun(i)\n",
        "            elif choice == 'scale':\n",
        "                scalefun(i)\n",
        "            elif choice == 'rotate':\n",
        "                rotatefun(i)\n",
        "            elif choice == 'noise':\n",
        "                noisefun(i)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "\n",
        "def get_daparam(dataset, model, model_eval, ipc):\n",
        "    # We find that augmentation doesn't always benefit the performance.\n",
        "    # So we do augmentation for some of the settings.\n",
        "\n",
        "    dc_aug_param = dict()\n",
        "    dc_aug_param['crop'] = 4\n",
        "    dc_aug_param['scale'] = 0.2\n",
        "    dc_aug_param['rotate'] = 45\n",
        "    dc_aug_param['noise'] = 0.001\n",
        "    dc_aug_param['strategy'] = 'none'\n",
        "\n",
        "    if dataset == 'MNIST':\n",
        "        dc_aug_param['strategy'] = 'crop_scale_rotate'\n",
        "\n",
        "    if model_eval in ['ConvNetBN']:  # Data augmentation makes model training with Batch Norm layer easier.\n",
        "        dc_aug_param['strategy'] = 'crop_noise'\n",
        "\n",
        "    return dc_aug_param\n",
        "\n",
        "\n",
        "def get_eval_pool(eval_mode, model, model_eval):\n",
        "    if eval_mode == 'M': # multiple architectures\n",
        "        # model_eval_pool = ['MLP', 'ConvNet', 'AlexNet', 'VGG11', 'ResNet18', 'LeNet']\n",
        "        model_eval_pool = ['ConvNet', 'AlexNet', 'VGG11', 'ResNet18']\n",
        "        # model_eval_pool = ['MLP', 'ConvNet', 'AlexNet', 'VGG11', 'ResNet18']\n",
        "    elif eval_mode == 'W': # ablation study on network width\n",
        "        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n",
        "    elif eval_mode == 'D': # ablation study on network depth\n",
        "        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n",
        "    elif eval_mode == 'A': # ablation study on network activation function\n",
        "        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL']\n",
        "    elif eval_mode == 'P': # ablation study on network pooling layer\n",
        "        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n",
        "    elif eval_mode == 'N': # ablation study on network normalization layer\n",
        "        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n",
        "    elif eval_mode == 'S': # itself\n",
        "        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n",
        "    elif eval_mode == 'C':\n",
        "        model_eval_pool = [model, 'ConvNet']\n",
        "    elif eval_mode == 'BN':\n",
        "        model_eval_pool = ['ConvNet','ConvNetBN','ResNet18','ResNet18BN','AlexNet', 'VGG11', 'ResNet18_AP']\n",
        "    else:\n",
        "        model_eval_pool = [model_eval]\n",
        "    return model_eval_pool\n",
        "\n",
        "\n",
        "class ParamDiffAug():\n",
        "    def __init__(self):\n",
        "        self.aug_mode = 'S' #'multiple or single'\n",
        "        self.prob_flip = 0.5\n",
        "        self.ratio_scale = 1.2\n",
        "        self.ratio_rotate = 15.0\n",
        "        self.ratio_crop_pad = 0.125\n",
        "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
        "        self.ratio_noise = 0.05\n",
        "        self.brightness = 1.0\n",
        "        self.saturation = 2.0\n",
        "        self.contrast = 0.5\n",
        "\n",
        "\n",
        "def set_seed_DiffAug(param):\n",
        "    if param.latestseed == -1:\n",
        "        return\n",
        "    else:\n",
        "        torch.random.manual_seed(param.latestseed)\n",
        "        param.latestseed += 1\n",
        "\n",
        "\n",
        "def DiffAugment(x, strategy='', seed = -1, param = None):\n",
        "    if seed == -1:\n",
        "        param.batchmode = False\n",
        "    else:\n",
        "        param.batchmode = True\n",
        "\n",
        "    param.latestseed = seed\n",
        "\n",
        "    if strategy == 'None' or strategy == 'none':\n",
        "        return x\n",
        "\n",
        "    if strategy:\n",
        "        if param.aug_mode == 'M': # original\n",
        "            for p in strategy.split('_'):\n",
        "                for f in AUGMENT_FNS[p]:\n",
        "                    x = f(x, param)\n",
        "        elif param.aug_mode == 'S':\n",
        "            pbties = strategy.split('_')\n",
        "            set_seed_DiffAug(param)\n",
        "            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n",
        "            for f in AUGMENT_FNS[p]:\n",
        "                x = f(x, param)\n",
        "        else:\n",
        "            exit('Error ZH: unknown augmentation mode.')\n",
        "        x = x.contiguous()\n",
        "    return x\n",
        "\n",
        "\n",
        "# We implement the following differentiable augmentation strategies based on the code provided in https://github.com/mit-han-lab/data-efficient-gans.\n",
        "def rand_scale(x, param):\n",
        "    # x>1, max scale\n",
        "    # sx, sy: (0, +oo), 1: orignial size, 0.5: enlarge 2 times\n",
        "    ratio = param.ratio_scale\n",
        "    set_seed_DiffAug(param)\n",
        "    sx = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
        "    set_seed_DiffAug(param)\n",
        "    sy = torch.rand(x.shape[0]) * (ratio - 1.0/ratio) + 1.0/ratio\n",
        "    theta = [[[sx[i], 0,  0],\n",
        "            [0,  sy[i], 0],] for i in range(x.shape[0])]\n",
        "    theta = torch.tensor(theta, dtype=torch.float)\n",
        "    if param.batchmode: # batch-wise:\n",
        "        theta[:] = theta[0]\n",
        "    grid = F.affine_grid(theta, x.shape, align_corners=True).to(x.device)\n",
        "    x = F.grid_sample(x, grid, align_corners=True)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_rotate(x, param): # [-180, 180], 90: anticlockwise 90 degree\n",
        "    ratio = param.ratio_rotate\n",
        "    set_seed_DiffAug(param)\n",
        "    theta = (torch.rand(x.shape[0]) - 0.5) * 2 * ratio / 180 * float(np.pi)\n",
        "    theta = [[[torch.cos(theta[i]), torch.sin(-theta[i]), 0],\n",
        "        [torch.sin(theta[i]), torch.cos(theta[i]),  0],]  for i in range(x.shape[0])]\n",
        "    theta = torch.tensor(theta, dtype=torch.float)\n",
        "    if param.batchmode: # batch-wise:\n",
        "        theta[:] = theta[0]\n",
        "    grid = F.affine_grid(theta, x.shape, align_corners=True).to(x.device)\n",
        "    x = F.grid_sample(x, grid, align_corners=True)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_flip(x, param):\n",
        "    prob = param.prob_flip\n",
        "    set_seed_DiffAug(param)\n",
        "    randf = torch.rand(x.size(0), 1, 1, 1, device=x.device)\n",
        "    if param.batchmode: # batch-wise:\n",
        "        randf[:] = randf[0]\n",
        "    return torch.where(randf < prob, x.flip(3), x)\n",
        "\n",
        "\n",
        "def rand_brightness(x, param):\n",
        "    ratio = param.brightness\n",
        "    set_seed_DiffAug(param)\n",
        "    randb = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.batchmode:  # batch-wise:\n",
        "        randb[:] = randb[0]\n",
        "    x = x + (randb - 0.5)*ratio\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_saturation(x, param):\n",
        "    ratio = param.saturation\n",
        "    x_mean = x.mean(dim=1, keepdim=True)\n",
        "    set_seed_DiffAug(param)\n",
        "    rands = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.batchmode:  # batch-wise:\n",
        "        rands[:] = rands[0]\n",
        "    x = (x - x_mean) * (rands * ratio) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_contrast(x, param):\n",
        "    ratio = param.contrast\n",
        "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
        "    set_seed_DiffAug(param)\n",
        "    randc = torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device)\n",
        "    if param.batchmode:  # batch-wise:\n",
        "        randc[:] = randc[0]\n",
        "    x = (x - x_mean) * (randc + ratio) + x_mean\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_crop(x, param):\n",
        "    # The image is padded on its surrounding and then cropped.\n",
        "    ratio = param.ratio_crop_pad\n",
        "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    set_seed_DiffAug(param)\n",
        "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    set_seed_DiffAug(param)\n",
        "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
        "    if param.batchmode:  # batch-wise:\n",
        "        translation_x[:] = translation_x[0]\n",
        "        translation_y[:] = translation_y[0]\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
        "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
        "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
        "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
        "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
        "    return x\n",
        "\n",
        "\n",
        "def rand_cutout(x, param):\n",
        "    ratio = param.ratio_cutout\n",
        "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
        "    set_seed_DiffAug(param)\n",
        "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    set_seed_DiffAug(param)\n",
        "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
        "    if param.batchmode:  # batch-wise:\n",
        "        offset_x[:] = offset_x[0]\n",
        "        offset_y[:] = offset_y[0]\n",
        "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
        "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
        "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
        "    )\n",
        "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
        "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
        "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
        "    mask[grid_batch, grid_x, grid_y] = 0\n",
        "    x = x * mask.unsqueeze(1)\n",
        "    return x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XzehnkTlWMa",
        "outputId": "53b5b7ad-9ff6-4411-ee08-78b6a49dd979"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-78769ef6f89d>:11: DeprecationWarning: Please use `rotate` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
            "  from scipy.ndimage.interpolation import rotate as scipyrotate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import warnings\n",
        "import types\n",
        "from collections import namedtuple\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "class ReparamModule(nn.Module):\n",
        "    def _get_module_from_name(self, mn):\n",
        "        if mn == '':\n",
        "            return self\n",
        "        m = self\n",
        "        for p in mn.split('.'):\n",
        "            m = getattr(m, p)\n",
        "        return m\n",
        "\n",
        "    def __init__(self, module):\n",
        "        super(ReparamModule, self).__init__()\n",
        "        self.module = module\n",
        "\n",
        "        param_infos = []  # (module name/path, param name)\n",
        "        shared_param_memo = {}\n",
        "        shared_param_infos = []  # (module name/path, param name, src module name/path, src param_name)\n",
        "        params = []\n",
        "        param_numels = []\n",
        "        param_shapes = []\n",
        "        for mn, m in self.named_modules():\n",
        "            for n, p in m.named_parameters(recurse=False):\n",
        "                if p is not None:\n",
        "                    if p in shared_param_memo:\n",
        "                        shared_mn, shared_n = shared_param_memo[p]\n",
        "                        shared_param_infos.append((mn, n, shared_mn, shared_n))\n",
        "                    else:\n",
        "                        shared_param_memo[p] = (mn, n)\n",
        "                        param_infos.append((mn, n))\n",
        "                        params.append(p.detach())\n",
        "                        param_numels.append(p.numel())\n",
        "                        param_shapes.append(p.size())\n",
        "\n",
        "        assert len(set(p.dtype for p in params)) <= 1, \\\n",
        "            \"expects all parameters in module to have same dtype\"\n",
        "\n",
        "        # store the info for unflatten\n",
        "        self._param_infos = tuple(param_infos)\n",
        "        self._shared_param_infos = tuple(shared_param_infos)\n",
        "        self._param_numels = tuple(param_numels)\n",
        "        self._param_shapes = tuple(param_shapes)\n",
        "\n",
        "        # flatten\n",
        "        flat_param = nn.Parameter(torch.cat([p.reshape(-1) for p in params], 0))\n",
        "        self.register_parameter('flat_param', flat_param)\n",
        "        self.param_numel = flat_param.numel()\n",
        "        del params\n",
        "        del shared_param_memo\n",
        "\n",
        "        # deregister the names as parameters\n",
        "        for mn, n in self._param_infos:\n",
        "            delattr(self._get_module_from_name(mn), n)\n",
        "        for mn, n, _, _ in self._shared_param_infos:\n",
        "            delattr(self._get_module_from_name(mn), n)\n",
        "\n",
        "        # register the views as plain attributes\n",
        "        self._unflatten_param(self.flat_param)\n",
        "\n",
        "        # now buffers\n",
        "        # they are not reparametrized. just store info as (module, name, buffer)\n",
        "        buffer_infos = []\n",
        "        for mn, m in self.named_modules():\n",
        "            for n, b in m.named_buffers(recurse=False):\n",
        "                if b is not None:\n",
        "                    buffer_infos.append((mn, n, b))\n",
        "\n",
        "        self._buffer_infos = tuple(buffer_infos)\n",
        "        self._traced_self = None\n",
        "\n",
        "    def trace(self, example_input, **trace_kwargs):\n",
        "        assert self._traced_self is None, 'This ReparamModule is already traced'\n",
        "\n",
        "        if isinstance(example_input, torch.Tensor):\n",
        "            example_input = (example_input,)\n",
        "        example_input = tuple(example_input)\n",
        "        example_param = (self.flat_param.detach().clone(),)\n",
        "        example_buffers = (tuple(b.detach().clone() for _, _, b in self._buffer_infos),)\n",
        "\n",
        "        self._traced_self = torch.jit.trace_module(\n",
        "            self,\n",
        "            inputs=dict(\n",
        "                _forward_with_param=example_param + example_input,\n",
        "                _forward_with_param_and_buffers=example_param + example_buffers + example_input,\n",
        "            ),\n",
        "            **trace_kwargs,\n",
        "        )\n",
        "\n",
        "        # replace forwards with traced versions\n",
        "        self._forward_with_param = self._traced_self._forward_with_param\n",
        "        self._forward_with_param_and_buffers = self._traced_self._forward_with_param_and_buffers\n",
        "        return self\n",
        "\n",
        "    def clear_views(self):\n",
        "        for mn, n in self._param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, None)  # This will set as plain attr\n",
        "\n",
        "    def _apply(self, *args, **kwargs):\n",
        "        if self._traced_self is not None:\n",
        "            self._traced_self._apply(*args, **kwargs)\n",
        "            return self\n",
        "        return super(ReparamModule, self)._apply(*args, **kwargs)\n",
        "\n",
        "    def _unflatten_param(self, flat_param):\n",
        "        ps = (t.view(s) for (t, s) in zip(flat_param.split(self._param_numels), self._param_shapes))\n",
        "        for (mn, n), p in zip(self._param_infos, ps):\n",
        "            setattr(self._get_module_from_name(mn), n, p)  # This will set as plain attr\n",
        "        for (mn, n, shared_mn, shared_n) in self._shared_param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, getattr(self._get_module_from_name(shared_mn), shared_n))\n",
        "\n",
        "    @contextmanager\n",
        "    def unflattened_param(self, flat_param):\n",
        "        saved_views = [getattr(self._get_module_from_name(mn), n) for mn, n in self._param_infos]\n",
        "        self._unflatten_param(flat_param)\n",
        "        yield\n",
        "        # Why not just `self._unflatten_param(self.flat_param)`?\n",
        "        # 1. because of https://github.com/pytorch/pytorch/issues/17583\n",
        "        # 2. slightly faster since it does not require reconstruct the split+view\n",
        "        #    graph\n",
        "        for (mn, n), p in zip(self._param_infos, saved_views):\n",
        "            setattr(self._get_module_from_name(mn), n, p)\n",
        "        for (mn, n, shared_mn, shared_n) in self._shared_param_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, getattr(self._get_module_from_name(shared_mn), shared_n))\n",
        "\n",
        "    @contextmanager\n",
        "    def replaced_buffers(self, buffers):\n",
        "        for (mn, n, _), new_b in zip(self._buffer_infos, buffers):\n",
        "            setattr(self._get_module_from_name(mn), n, new_b)\n",
        "        yield\n",
        "        for mn, n, old_b in self._buffer_infos:\n",
        "            setattr(self._get_module_from_name(mn), n, old_b)\n",
        "\n",
        "    def _forward_with_param_and_buffers(self, flat_param, buffers, *inputs, **kwinputs):\n",
        "        with self.unflattened_param(flat_param):\n",
        "            with self.replaced_buffers(buffers):\n",
        "                return self.module(*inputs, **kwinputs)\n",
        "\n",
        "    def _forward_with_param(self, flat_param, *inputs, **kwinputs):\n",
        "        with self.unflattened_param(flat_param):\n",
        "            return self.module(*inputs, **kwinputs)\n",
        "\n",
        "    def forward(self, *inputs, flat_param=None, buffers=None, **kwinputs):\n",
        "        flat_param = torch.squeeze(flat_param)\n",
        "        # print(\"PARAMS ON DEVICE: \", flat_param.get_device())\n",
        "        # print(\"DATA ON DEVICE: \", inputs[0].get_device())\n",
        "        # flat_param.to(\"cuda:{}\".format(inputs[0].get_device()))\n",
        "        # self.module.to(\"cuda:{}\".format(inputs[0].get_device()))\n",
        "        if flat_param is None:\n",
        "            flat_param = self.flat_param\n",
        "        if buffers is None:\n",
        "            return self._forward_with_param(flat_param, *inputs, **kwinputs)\n",
        "        else:\n",
        "            return self._forward_with_param_and_buffers(flat_param, tuple(buffers), *inputs, **kwinputs)"
      ],
      "metadata": {
        "id": "ks7Luyb0nA2l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya23sTYVoKBT",
        "outputId": "6cdada93-3e00-4f35-e02b-7b469d09a74b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.38.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils\n",
        "from tqdm import tqdm\n",
        "#from utils.utils_baseline import get_dataset, get_network, get_eval_pool, evaluate_synset, get_time, DiffAugment, ParamDiffAug\n",
        "import wandb\n",
        "import copy\n",
        "import random\n",
        "#from reparam_module import ReparamModule\n",
        "# from kmeans_pytorch import kmeans\n",
        "#from utils.cfg import CFG as cfg\n",
        "import warnings\n",
        "import yaml\n",
        "\n"
      ],
      "metadata": {
        "id": "ra34wyhKlGEE"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform the Distillation"
      ],
      "metadata": {
        "id": "jlZkmqZSmYLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzlwHGcjttrw",
        "outputId": "953713e3-661b-4a19-edb8-321f45ee4ccf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.40)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.38.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And more packages & functions"
      ],
      "metadata": {
        "id": "_IOjDySs2nyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "import copy\n",
        "import random\n",
        "import kornia as K\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.utils\n",
        "from tqdm import tqdm\n",
        "from utils_baseline import get_dataset, get_network, get_eval_pool, evaluate_synset, get_time, DiffAugment, ParamDiffAug\n",
        "import wandb\n",
        "import copy\n",
        "import random\n",
        "from reparam_module import ReparamModule\n",
        "# from kmeans_pytorch import kmeans\n",
        "from cfg import CFG as cfg\n",
        "import warnings\n",
        "import yaml\n",
        "\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)\n",
        "torch.cuda.manual_seed_all(1)"
      ],
      "metadata": {
        "id": "IufnELZYtx6w"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "uorrAf36fP0s"
      },
      "outputs": [],
      "source": [
        "######\n",
        "#https://github.com/EricFeng20001120/ECE1512_2023F_ProjectRepo_-YixinFeng/blob/main/Project_B/Task2.ipynb\n",
        "######\n",
        "args = type('', (), {})()\n",
        "\n",
        "args.dataset = 'MNIST'\n",
        "args.subset = 'imagenette'\n",
        "args.model = 'ConvNet'\n",
        "args.ipc = 5\n",
        "args.eval_mode = 'S'\n",
        "args.num_eval = 3\n",
        "args.eval_it = 100\n",
        "args.epoch_eval_train = 1000\n",
        "args.Iteration = 300\n",
        "args.lr_img = 1000\n",
        "args.lr_teacher = 0.01\n",
        "args.lr_init = 0.01\n",
        "args.batch_real = 256\n",
        "args.batch_syn = None\n",
        "args.batch_train = 256\n",
        "args.pix_init = 'samples_predicted_correctly'  # initialize synthetic images from random noise or real images\n",
        "args.dsa = True\n",
        "args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate'  # differentiable Siamese augmentation strategy\n",
        "if args.dsa:\n",
        "    # args.epoch_eval_train = 1000\n",
        "    args.dc_aug_param = None\n",
        "\n",
        "args.dsa_param = ParamDiffAug()\n",
        "\n",
        "dsa_params = args.dsa_param\n",
        "\n",
        "args.data_path = './data/'\n",
        "args.buffer_path = './buffers/'\n",
        "args.expert_epochs = 2\n",
        "args.syn_steps = 80\n",
        "args.max_start_epoch = 5\n",
        "args.min_start_epoch = 0\n",
        "args.zca = True\n",
        "args.load_all = False\n",
        "args.no_aug = False\n",
        "args.texture = False\n",
        "args.canvas_size = 2\n",
        "args.canvas_samples = 1\n",
        "args.max_files = None\n",
        "args.max_experts = None\n",
        "args.force_save = False\n",
        "args.ema_decay = 0.999\n",
        "args.lr_y = 5.\n",
        "args.Momentum_y = 0.9\n",
        "args.project = 'TEST'\n",
        "args.threshold = 1.0\n",
        "args.record_loss = False\n",
        "args.Sequential_Generation = True\n",
        "args.expansion_end_epoch = 3000\n",
        "args.current_max_start_epoch = 1\n",
        "\n",
        "args.skip_first_eva = True  # If skip first eva\n",
        "args.parall_eva = False  # If parallel eva\n",
        "args.lr_lr = 0.00001\n",
        "args.res = 32\n",
        "args.device = [0]\n",
        "\n",
        "args.Initialize_Label_With_Another_Model = False\n",
        "args.Initialize_Label_Model = \"\"\n",
        "args.Initialize_Label_Model_Dir = \"\"\n",
        "args.Label_Model_Timestamp = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "e5_xrUfZPAK2"
      },
      "outputs": [],
      "source": [
        "#########\n",
        "#https://github.com/GzyAftermath/DATM/blob/main/distill/DATM.py\n",
        "#########\n",
        "def DATM():\n",
        "  args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "  if args.skip_first_eva==False:\n",
        "      eval_it_pool = np.arange(0, args.Iteration + 1, args.eval_it).tolist()\n",
        "  else:\n",
        "      eval_it_pool = np.arange(args.eval_it, args.Iteration + 1, args.eval_it).tolist()\n",
        "  channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader, loader_train_dict, class_map, class_map_inv = get_dataset(args.dataset, args.data_path, args.batch_real, args.subset, args=args)\n",
        "  model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
        "\n",
        "  im_res = im_size[0]\n",
        "\n",
        "  args.im_size = im_size\n",
        "\n",
        "  accs_all_exps = dict() # record performances of all experiments\n",
        "  for key in model_eval_pool:\n",
        "      accs_all_exps[key] = []\n",
        "\n",
        "  data_save = []\n",
        "\n",
        "  if args.dsa:\n",
        "      # args.epoch_eval_train = 1000\n",
        "      args.dc_aug_param = None\n",
        "\n",
        "  args.dsa_param = ParamDiffAug()\n",
        "\n",
        "  dsa_params = args.dsa_param\n",
        "  if args.zca:\n",
        "      zca_trans = args.zca_trans\n",
        "  else:\n",
        "      zca_trans = None\n",
        "\n",
        "  wandb.init(sync_tensorboard=False,\n",
        "              project=args.project,\n",
        "              job_type=\"CleanRepo\",\n",
        "              config=args,\n",
        "              )\n",
        "\n",
        "  #args = type('', (), {})()\n",
        "\n",
        "  for key in wandb.config._items:\n",
        "      setattr(args, key, wandb.config._items[key])\n",
        "\n",
        "  args.dsa_param = dsa_params\n",
        "  args.zca_trans = zca_trans\n",
        "\n",
        "  if args.batch_syn is None:\n",
        "      args.batch_syn = num_classes * args.ipc\n",
        "\n",
        "  args.distributed = torch.cuda.device_count() > 1\n",
        "\n",
        "\n",
        "  print('Hyper-parameters: \\n', args.__dict__)\n",
        "  print('Evaluation model pool: ', model_eval_pool)\n",
        "\n",
        "  ''' organize the real dataset '''\n",
        "  images_all = []\n",
        "  labels_all = []\n",
        "  indices_class = [[] for c in range(num_classes)]\n",
        "  print(\"BUILDING DATASET\")\n",
        "  if args.dataset == 'ImageNet1K' and os.path.exists('images_all.pt') and os.path.exists('labels_all.pt'):\n",
        "      images_all = torch.load('images_all.pt')\n",
        "      labels_all = torch.load('labels_all.pt')\n",
        "  else:\n",
        "      for i in tqdm(range(len(dst_train))):\n",
        "          sample = dst_train[i]\n",
        "          images_all.append(torch.unsqueeze(sample[0], dim=0))\n",
        "          labels_all.append(class_map[torch.tensor(sample[1]).item()])\n",
        "      images_all = torch.cat(images_all, dim=0).to(\"cpu\")\n",
        "      labels_all = torch.tensor(labels_all, dtype=torch.long, device=\"cpu\")\n",
        "      if args.dataset == 'ImageNet1K':\n",
        "          torch.save(images_all, 'images_all.pt')\n",
        "          torch.save(labels_all, 'labels_all.pt')\n",
        "\n",
        "  for i, lab in tqdm(enumerate(labels_all)):\n",
        "      indices_class[lab].append(i)\n",
        "\n",
        "\n",
        "\n",
        "  for c in range(num_classes):\n",
        "      print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
        "\n",
        "  for ch in range(channel):\n",
        "      print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
        "\n",
        "\n",
        "  def get_images(c, n):  # get random n images from class c\n",
        "      idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
        "      return images_all[idx_shuffle]\n",
        "  ''' initialize the synthetic data '''\n",
        "  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n",
        "\n",
        "\n",
        "  image_syn = torch.randn(size=(num_classes * args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float)\n",
        "\n",
        "  syn_lr = torch.tensor(args.lr_teacher).to(args.device)\n",
        "  expert_dir = os.path.join(args.buffer_path, args.dataset)\n",
        "  if args.dataset == \"ImageNet\":\n",
        "    expert_dir = os.path.join(expert_dir, args.subset, str(args.res))\n",
        "  if args.dataset in [\"CIFAR10\", \"CIFAR100\"] and not args.zca:\n",
        "    expert_dir += \"_NO_ZCA\"\n",
        "  expert_dir = os.path.join(expert_dir, args.model)\n",
        "  print(\"Expert Dir: {}\".format(expert_dir))\n",
        "  if args.load_all:\n",
        "    buffer = []\n",
        "    n = 0\n",
        "    while os.path.exists(os.path.join(expert_dir, \"replay_buffer_{}.pt\".format(n))):\n",
        "        buffer = buffer + torch.load(os.path.join(expert_dir, \"replay_buffer_{}.pt\".format(n)))\n",
        "        n += 1\n",
        "    if n == 0:\n",
        "        raise AssertionError(\"No buffers detected at {}\".format(expert_dir))\n",
        "\n",
        "  else:\n",
        "    expert_files = []\n",
        "    n = 0\n",
        "    while os.path.exists(os.path.join(expert_dir, \"replay_buffer_{}.pt\".format(n))):\n",
        "        expert_files.append(os.path.join(expert_dir, \"replay_buffer_{}.pt\".format(n)))\n",
        "        n += 1\n",
        "    if n == 0:\n",
        "        raise AssertionError(\"No buffers detected at {}\".format(expert_dir))\n",
        "    file_idx = 0\n",
        "    expert_idx = 0\n",
        "    # random.shuffle(expert_files)\n",
        "    if args.max_files is not None:\n",
        "        expert_files = expert_files[:args.max_files]\n",
        "\n",
        "    expert_id = [i for i in range(len(expert_files))]\n",
        "    random.shuffle(expert_id)\n",
        "\n",
        "    print(\"loading file {}\".format(expert_files[expert_id[file_idx]]))\n",
        "    buffer = torch.load(expert_files[expert_id[file_idx]])\n",
        "    if args.max_experts is not None:\n",
        "        buffer = buffer[:args.max_experts]\n",
        "    buffer_id = [i for i in range(len(buffer))]\n",
        "    random.shuffle(buffer_id)\n",
        "\n",
        "    if args.pix_init == 'real':\n",
        "        print('initialize synthetic data from random real images')\n",
        "        for c in range(num_classes):\n",
        "            image_syn.data[c * args.ipc:(c + 1) * args.ipc] = get_images(c, args.ipc).detach().data\n",
        "\n",
        "    elif args.pix_init == 'samples_predicted_correctly':\n",
        "        if args.parall_eva==False:\n",
        "            device = torch.device(\"cuda:0\")\n",
        "        else:\n",
        "            device = args.device\n",
        "        if args.Initialize_Label_With_Another_Model:\n",
        "            Temp_net = get_network(args.Initialize_Label_Model, channel, num_classes, im_size, dist=False).to(device)  # get a random model\n",
        "        else:\n",
        "            Temp_net = get_network(args.model, channel, num_classes, im_size, dist=False).to(device)  # get a random model\n",
        "        Temp_net.eval()\n",
        "        Temp_net = ReparamModule(Temp_net)\n",
        "        if args.distributed and args.parall_eva==True:\n",
        "            Temp_net = torch.nn.DataParallel(Temp_net)\n",
        "        Temp_net.eval()\n",
        "        logits=[]\n",
        "        label_expert_files = expert_files\n",
        "        temp_params = torch.load(label_expert_files[0])[0][args.Label_Model_Timestamp]\n",
        "        temp_params = torch.cat([p.data.to(device).reshape(-1) for p in temp_params], 0)\n",
        "        if args.distributed and args.parall_eva==True:\n",
        "            temp_params = temp_params.unsqueeze(0).expand(torch.cuda.device_count(), -1)\n",
        "        for c in range(num_classes):\n",
        "            data_for_class_c = get_images(c, len(indices_class[c])).detach().data\n",
        "            n, _, w, h = data_for_class_c.shape\n",
        "            selected_num = 0\n",
        "            select_times = 0\n",
        "            cur=0\n",
        "            temp_img = None\n",
        "            Wrong_Predicted_Img = None\n",
        "            batch_size = 256\n",
        "            index = []\n",
        "            while len(index)<args.ipc:\n",
        "                print(str(c)+'.'+str(select_times)+'.'+str(cur))\n",
        "                current_data_batch = data_for_class_c[batch_size*select_times : batch_size*(select_times+1)].detach().to(device)\n",
        "                if batch_size*select_times > len(data_for_class_c):\n",
        "                    select_times = 0\n",
        "                    cur+=1\n",
        "                    temp_params = torch.load(label_expert_files[int(cur/10)%10])[cur%10][args.Label_Model_Timestamp]\n",
        "                    temp_params = torch.cat([p.data.to(device).reshape(-1) for p in temp_params], 0).to(device)\n",
        "                    if args.distributed and args.parall_eva==True:\n",
        "                        temp_params = temp_params.unsqueeze(0).expand(torch.cuda.device_count(), -1)\n",
        "                    continue\n",
        "                logits = Temp_net(current_data_batch, flat_param=temp_params).detach()\n",
        "                prediction_class = np.argmax(logits.cpu().data.numpy(), axis=-1)\n",
        "                for i in range(len(prediction_class)):\n",
        "                    if prediction_class[i]==c and len(index)<args.ipc:\n",
        "                        index.append(batch_size*select_times+i)\n",
        "                        index=list(set(index))\n",
        "                select_times+=1\n",
        "                if len(index) == args.ipc:\n",
        "                    temp_img = torch.index_select(data_for_class_c, dim=0, index=torch.tensor(index))\n",
        "                    break\n",
        "            image_syn.data[c * args.ipc:(c + 1) * args.ipc] = temp_img.detach()\n",
        "    else:\n",
        "        print('initialize synthetic data from random noise')\n",
        "\n",
        "    ''' training '''\n",
        "    image_syn = image_syn.detach().to(args.device).requires_grad_(True)\n",
        "    syn_lr = syn_lr.detach().to(args.device).requires_grad_(True)\n",
        "\n",
        "    #optimizer_img = torch.optim.SGD([image_syn], lr=args.lr_img, momentum=0.1)\n",
        "    #optimizer_lr = torch.optim.SGD([syn_lr], lr=args.lr_lr, momentum=0.1)\n",
        "\n",
        "    optimizer_img = torch.optim.SGD([image_syn], lr=args.lr_img, momentum=0.5)\n",
        "    optimizer_lr = torch.optim.SGD([syn_lr], lr=args.lr_lr, momentum=0.5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    optimizer_img.zero_grad()\n",
        "\n",
        "    ###\n",
        "\n",
        "    '''test'''\n",
        "    def SoftCrossEntropy(inputs, target, reduction='average'):\n",
        "        input_log_likelihood = -F.log_softmax(inputs, dim=1)\n",
        "        target_log_likelihood = F.softmax(target, dim=1)\n",
        "        batch = inputs.shape[0]\n",
        "        loss = torch.sum(torch.mul(input_log_likelihood, target_log_likelihood)) / batch\n",
        "        return loss\n",
        "\n",
        "    criterion = SoftCrossEntropy\n",
        "\n",
        "    print('%s training begins'%get_time())\n",
        "    best_acc = {m: 0 for m in model_eval_pool}\n",
        "    best_std = {m: 0 for m in model_eval_pool}\n",
        "\n",
        "    '''------test------'''\n",
        "    '''only sum correct predicted logits'''\n",
        "    if args.pix_init == \"samples_predicted_correctly\":\n",
        "        if args.Initialize_Label_With_Another_Model:\n",
        "            Temp_net = get_network(args.Initialize_Label_Model, channel, num_classes, im_size, dist=False).to(device)  # get a random model\n",
        "        else:\n",
        "            Temp_net = get_network(args.model, channel, num_classes, im_size, dist=False).to(device)  # get a random model\n",
        "        Temp_net.eval()\n",
        "        Temp_net = ReparamModule(Temp_net)\n",
        "        if args.distributed:\n",
        "            Temp_net = torch.nn.DataParallel(Temp_net)\n",
        "        Temp_net.eval()\n",
        "        logits=[]\n",
        "        batch_size = 256\n",
        "        for i in range(len(label_expert_files)):\n",
        "            Temp_Buffer = torch.load(label_expert_files[i])\n",
        "            for j in Temp_Buffer:\n",
        "                temp_logits = None\n",
        "                for select_times in range((len(image_syn)+batch_size-1)//batch_size):\n",
        "                    current_data_batch = image_syn[batch_size*select_times : batch_size*(select_times+1)].detach().to(device)\n",
        "                    Temp_params = j[args.Label_Model_Timestamp]\n",
        "                    Initialize_Labels_params = torch.cat([p.data.to(args.device).reshape(-1) for p in Temp_params], 0)\n",
        "                    if args.distributed:\n",
        "                        Initialize_Labels_params = Initialize_Labels_params.unsqueeze(0).expand(torch.cuda.device_count(), -1)\n",
        "                    Initialized_Labels = Temp_net(current_data_batch, flat_param=Initialize_Labels_params)\n",
        "                    if temp_logits == None:\n",
        "                        temp_logits = Initialized_Labels.detach()\n",
        "                    else:\n",
        "                        temp_logits = torch.cat((temp_logits, Initialized_Labels.detach()),0)\n",
        "                logits.append(temp_logits.detach().cpu())\n",
        "        logits_tensor = torch.stack(logits)\n",
        "        true_labels = label_syn.cpu()\n",
        "        predicted_labels = torch.argmax(logits_tensor, dim=2).cpu()\n",
        "        correct_predictions = predicted_labels == true_labels.view(1, -1)\n",
        "        mask = correct_predictions.unsqueeze(2)\n",
        "        correct_logits = logits_tensor * mask.float()\n",
        "        correct_logits_per_model = correct_logits.sum(dim=0)\n",
        "        num_correct_images_per_model = correct_predictions.sum(dim=0, dtype=torch.float)\n",
        "        average_logits_per_image = correct_logits_per_model / num_correct_images_per_model.unsqueeze(1)\n",
        "        Initialized_Labels = average_logits_per_image\n",
        "\n",
        "    elif args.pix_init == \"real\":\n",
        "        Temp_net = get_network(args.model, channel, num_classes, im_size, dist=False).to(args.device)  # get a random model\n",
        "        Temp_net = ReparamModule(Temp_net)\n",
        "        if args.distributed:\n",
        "            Temp_net = torch.nn.DataParallel(Temp_net)\n",
        "        Temp_net.eval()\n",
        "        Temp_params = buffer[0][-1]\n",
        "        Initialize_Labels_params = torch.cat([p.data.to(args.device).reshape(-1) for p in Temp_params], 0)\n",
        "        if args.distributed:\n",
        "            Initialize_Labels_params = Initialize_Labels_params.unsqueeze(0).expand(torch.cuda.device_count(), -1)\n",
        "        Initialized_Labels = Temp_net(image_syn, flat_param=Initialize_Labels_params)\n",
        "\n",
        "    acc = np.sum(np.equal(np.argmax(Initialized_Labels.cpu().data.numpy(), axis=-1), label_syn.cpu().data.numpy()))\n",
        "    print('InitialAcc:{}'.format(acc/len(label_syn)))\n",
        "\n",
        "    label_syn = copy.deepcopy(Initialized_Labels.detach()).to(args.device).requires_grad_(True)\n",
        "    label_syn.requires_grad=True\n",
        "    label_syn = label_syn.to(args.device)\n",
        "\n",
        "\n",
        "    optimizer_y = torch.optim.SGD([label_syn], lr=args.lr_y, momentum=args.Momentum_y)\n",
        "    vs = torch.zeros_like(label_syn)\n",
        "    accumulated_grad = torch.zeros_like(label_syn)\n",
        "    last_random = 0\n",
        "\n",
        "    del Temp_net\n",
        "\n",
        "    # test\n",
        "    curMax_times = 0\n",
        "    current_accumulated_step = 0\n",
        "\n",
        "    for it in range(0, args.Iteration+1):\n",
        "        save_this_it = False\n",
        "        wandb.log({\"Progress\": it}, step=it)\n",
        "        ''' Evaluate synthetic data '''\n",
        "        if it in eval_it_pool:\n",
        "            for model_eval in model_eval_pool:\n",
        "                print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d'%(args.model, model_eval, it))\n",
        "                if args.dsa:\n",
        "                    print('DSA augmentation strategy: \\n', args.dsa_strategy)\n",
        "                    print('DSA augmentation parameters: \\n', args.dsa_param.__dict__)\n",
        "                else:\n",
        "                    print('DC augmentation parameters: \\n', args.dc_aug_param)\n",
        "\n",
        "                accs_test = []\n",
        "                accs_train = []\n",
        "\n",
        "                for it_eval in range(args.num_eval):\n",
        "                    if args.parall_eva==False:\n",
        "                        device = torch.device(\"cuda:0\")\n",
        "                        net_eval = get_network(model_eval, channel, num_classes, im_size, dist=False).to(device) # get a random model\n",
        "                    else:\n",
        "                        device = args.device\n",
        "                        net_eval = get_network(model_eval, channel, num_classes, im_size, dist=True).to(device) # get a random model\n",
        "\n",
        "                    eval_labs = label_syn.detach().to(device)\n",
        "                    with torch.no_grad():\n",
        "                        image_save = image_syn.to(device)\n",
        "                    image_syn_eval, label_syn_eval = copy.deepcopy(image_save.detach()).to(device), copy.deepcopy(eval_labs.detach()).to(device) # avoid any unaware modification\n",
        "\n",
        "                    args.lr_net = syn_lr.item()\n",
        "                    _, acc_train, acc_test = evaluate_synset(it_eval, copy.deepcopy(net_eval).to(device), image_syn_eval.to(device), label_syn_eval.to(device), testloader, args, texture=False, train_criterion=criterion)\n",
        "                    accs_test.append(acc_test)\n",
        "                    accs_train.append(acc_train)\n",
        "\n",
        "                accs_test = np.array(accs_test)\n",
        "                accs_train = np.array(accs_train)\n",
        "                acc_test_mean = np.mean(accs_test)\n",
        "                acc_test_std = np.std(accs_test)\n",
        "\n",
        "                if acc_test_mean > best_acc[model_eval]:\n",
        "                    best_acc[model_eval] = acc_test_mean\n",
        "                    best_std[model_eval] = acc_test_std\n",
        "                    save_this_it = True\n",
        "                print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs_test), model_eval, acc_test_mean, acc_test_std))\n",
        "                wandb.log({'Accuracy/{}'.format(model_eval): acc_test_mean}, step=it)\n",
        "                wandb.log({'Max_Accuracy/{}'.format(model_eval): best_acc[model_eval]}, step=it)\n",
        "                wandb.log({'Std/{}'.format(model_eval): acc_test_std}, step=it)\n",
        "                wandb.log({'Max_Std/{}'.format(model_eval): best_std[model_eval]}, step=it)\n",
        "\n",
        "        if it in eval_it_pool and (save_this_it or it % 1000 == 0):\n",
        "            with torch.no_grad():\n",
        "                image_save = image_syn.cuda()\n",
        "                save_dir = os.path.join(\".\", \"logged_files\", args.dataset, str(args.ipc), args.model, wandb.run.name)\n",
        "\n",
        "                if not os.path.exists(save_dir):\n",
        "                    os.makedirs(os.path.join(save_dir,'Normal'))\n",
        "\n",
        "                torch.save(image_save.cpu(), os.path.join(save_dir, 'Normal',\"images_{}.pt\".format(it)))\n",
        "                torch.save(label_syn.cpu(), os.path.join(save_dir, 'Normal', \"labels_{}.pt\".format(it)))\n",
        "                torch.save(syn_lr.detach().cpu(), os.path.join(save_dir, 'Normal', \"lr_{}.pt\".format(it)))\n",
        "\n",
        "                if save_this_it:\n",
        "                    torch.save(image_save.cpu(), os.path.join(save_dir, 'Normal', \"images_best.pt\".format(it)))\n",
        "                    torch.save(label_syn.cpu(), os.path.join(save_dir, 'Normal', \"labels_best.pt\".format(it)))\n",
        "                    torch.save(syn_lr.detach().cpu(), os.path.join(save_dir, 'Normal', \"lr_best.pt\".format(it)))\n",
        "\n",
        "                wandb.log({\"Pixels\": wandb.Histogram(torch.nan_to_num(image_syn.detach().cpu()))}, step=it)\n",
        "\n",
        "                if args.ipc < 50 or args.force_save:\n",
        "                    upsampled = image_save\n",
        "                    if args.dataset != \"ImageNet\":\n",
        "                        upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=2)\n",
        "                        upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=3)\n",
        "                    grid = torchvision.utils.make_grid(upsampled, nrow=10, normalize=True, scale_each=True)\n",
        "                    wandb.log({\"Synthetic_Images\": wandb.Image(torch.nan_to_num(grid.detach().cpu()))}, step=it)\n",
        "                    wandb.log({'Synthetic_Pixels': wandb.Histogram(torch.nan_to_num(image_save.detach().cpu()))}, step=it)\n",
        "\n",
        "                    for clip_val in [2.5]:\n",
        "                        std = torch.std(image_save)\n",
        "                        mean = torch.mean(image_save)\n",
        "                        upsampled = torch.clip(image_save, min=mean-clip_val*std, max=mean+clip_val*std)\n",
        "                        if args.dataset != \"ImageNet\":\n",
        "                            upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=2)\n",
        "                            upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=3)\n",
        "                        grid = torchvision.utils.make_grid(upsampled, nrow=10, normalize=True, scale_each=True)\n",
        "                        wandb.log({\"Clipped_Synthetic_Images/std_{}\".format(clip_val): wandb.Image(torch.nan_to_num(grid.detach().cpu()))}, step=it)\n",
        "\n",
        "                    if args.zca:\n",
        "                        image_save = image_save.to(args.device)\n",
        "                        image_save = args.zca_trans.inverse_transform(image_save)\n",
        "                        image_save.cpu()\n",
        "                        torch.save(image_save.cpu(), os.path.join(save_dir, 'Normal', \"images_zca_{}.pt\".format(it)))\n",
        "                        upsampled = image_save\n",
        "                        if args.dataset != \"ImageNet\":\n",
        "                            upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=2)\n",
        "                            upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=3)\n",
        "                        grid = torchvision.utils.make_grid(upsampled, nrow=10, normalize=True, scale_each=True)\n",
        "                        wandb.log({\"Reconstructed_Images\": wandb.Image(torch.nan_to_num(grid.detach().cpu()))}, step=it)\n",
        "                        wandb.log({'Reconstructed_Pixels': wandb.Histogram(torch.nan_to_num(image_save.detach().cpu()))}, step=it)\n",
        "                        for clip_val in [2.5]:\n",
        "                            std = torch.std(image_save)\n",
        "                            mean = torch.mean(image_save)\n",
        "                            upsampled = torch.clip(image_save, min=mean - clip_val * std, max=mean + clip_val * std)\n",
        "                            if args.dataset != \"ImageNet\":\n",
        "                                upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=2)\n",
        "                                upsampled = torch.repeat_interleave(upsampled, repeats=4, dim=3)\n",
        "                            grid = torchvision.utils.make_grid(upsampled, nrow=10, normalize=True, scale_each=True)\n",
        "                            wandb.log({\"Clipped_Reconstructed_Images/std_{}\".format(clip_val): wandb.Image(\n",
        "                                torch.nan_to_num(grid.detach().cpu()))}, step=it)\n",
        "\n",
        "\n",
        "\n",
        "        wandb.log({\"Synthetic_LR\": syn_lr.detach().cpu()}, step=it)\n",
        "\n",
        "        student_net = get_network(args.model, channel, num_classes, im_size, dist=False).to(args.device)  # get a random model\n",
        "\n",
        "        student_net = ReparamModule(student_net)\n",
        "\n",
        "        if args.distributed:\n",
        "            student_net = torch.nn.DataParallel(student_net)\n",
        "\n",
        "        student_net.train()\n",
        "\n",
        "        num_params = sum([np.prod(p.size()) for p in (student_net.parameters())])\n",
        "\n",
        "        if args.load_all:\n",
        "            expert_trajectory = buffer[np.random.randint(0, len(buffer))]\n",
        "        else:\n",
        "            expert_trajectory = buffer[buffer_id[expert_idx]]\n",
        "            expert_idx += 1\n",
        "            if expert_idx == len(buffer):\n",
        "                expert_idx = 0\n",
        "                file_idx += 1\n",
        "                if file_idx == len(expert_files):\n",
        "                    file_idx = 0\n",
        "                    random.shuffle(expert_id)\n",
        "                print(\"loading file {}\".format(expert_files[expert_id[file_idx]]))\n",
        "                if args.max_files != 1:\n",
        "                    del buffer\n",
        "                    buffer = torch.load(expert_files[expert_id[file_idx]])\n",
        "                if args.max_experts is not None:\n",
        "                    buffer = buffer[:args.max_experts]\n",
        "                random.shuffle(buffer_id)\n",
        "\n",
        "        # Only match easy traj. in the early stage\n",
        "        if args.Sequential_Generation:\n",
        "            Upper_Bound = args.current_max_start_epoch + int((args.max_start_epoch-args.current_max_start_epoch) * it/(args.expansion_end_epoch))\n",
        "            Upper_Bound = min(Upper_Bound, args.max_start_epoch)\n",
        "        else:\n",
        "            Upper_Bound = args.max_start_epoch\n",
        "\n",
        "        start_epoch = np.random.randint(args.min_start_epoch, Upper_Bound)\n",
        "\n",
        "        starting_params = expert_trajectory[start_epoch]\n",
        "        target_params = expert_trajectory[start_epoch+args.expert_epochs]\n",
        "        target_params = torch.cat([p.data.to(args.device).reshape(-1) for p in target_params], 0)\n",
        "        student_params = [torch.cat([p.data.to(args.device).reshape(-1) for p in starting_params], 0).requires_grad_(True)]\n",
        "        starting_params = torch.cat([p.data.to(args.device).reshape(-1) for p in starting_params], 0)\n",
        "\n",
        "        syn_images = image_syn\n",
        "        y_hat = label_syn\n",
        "\n",
        "        param_loss_list = []\n",
        "        param_dist_list = []\n",
        "        indices_chunks = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for step in range(args.syn_steps):\n",
        "            if not indices_chunks:\n",
        "                indices = torch.randperm(len(syn_images))\n",
        "                indices_chunks = list(torch.split(indices, args.batch_syn))\n",
        "\n",
        "            these_indices = indices_chunks.pop()\n",
        "\n",
        "            x = syn_images[these_indices]\n",
        "            this_y = y_hat[these_indices]\n",
        "\n",
        "\n",
        "            if args.dsa and (not args.no_aug):\n",
        "                x = DiffAugment(x, args.dsa_strategy, param=args.dsa_param)\n",
        "\n",
        "            if args.distributed:\n",
        "                forward_params = student_params[-1].unsqueeze(0).expand(torch.cuda.device_count(), -1)\n",
        "            else:\n",
        "                forward_params = student_params[-1]\n",
        "            x = student_net(x, flat_param=forward_params)\n",
        "            ce_loss = criterion(x, this_y)\n",
        "\n",
        "            grad = torch.autograd.grad(ce_loss, student_params[-1], create_graph=True)[0]\n",
        "\n",
        "            student_params.append(student_params[-1] - syn_lr * grad)\n",
        "\n",
        "        param_loss = torch.tensor(0.0).to(args.device)\n",
        "        param_dist = torch.tensor(0.0).to(args.device)\n",
        "\n",
        "        param_loss += torch.nn.functional.mse_loss(student_params[-1], target_params, reduction=\"sum\")\n",
        "        param_dist += torch.nn.functional.mse_loss(starting_params, target_params, reduction=\"sum\")\n",
        "\n",
        "        param_loss_list.append(param_loss)\n",
        "        param_dist_list.append(param_dist)\n",
        "\n",
        "        param_loss /= num_params\n",
        "        param_dist /= num_params\n",
        "\n",
        "        param_loss /= param_dist\n",
        "\n",
        "        grand_loss = param_loss\n",
        "\n",
        "        optimizer_img.zero_grad()\n",
        "        optimizer_lr.zero_grad()\n",
        "        optimizer_y.zero_grad()\n",
        "\n",
        "        grand_loss.backward()\n",
        "\n",
        "        if grand_loss<=args.threshold:\n",
        "            optimizer_y.step()\n",
        "            optimizer_img.step()\n",
        "            optimizer_lr.step()\n",
        "        else:\n",
        "            wandb.log({\"falts\": start_epoch}, step=it)\n",
        "\n",
        "\n",
        "\n",
        "        wandb.log({\"Grand_Loss\": param_loss.detach().cpu(),\n",
        "                    \"Start_Epoch\": start_epoch})\n",
        "\n",
        "        for _ in student_params:\n",
        "            del _\n",
        "\n",
        "        if it%10 == 0:\n",
        "            print('%s iter = %04d, loss = %.4f' % (get_time(), it, grand_loss.item()))\n",
        "\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "sIX1SQCQ2uQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATM()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "40edb8155df147fc9a290c444ccf380d",
            "933241d07ed1412abc3613eb1a4e3ac3",
            "b472a3f19456458394767b328f6412a2",
            "b130aec329e845a58125a13a34a17efc",
            "c7484565cbfa40f68b7588f3ed51b8ee",
            "f567b2c58bd841b2998bcfbeb0d7e6a3",
            "a3aa5ce1591a4432a4cff5445d780176",
            "5f80383b278a47f4b26629c32d9930a3"
          ]
        },
        "id": "p_kY4KLCuiDk",
        "outputId": "a1548e09-e5f9-4576-b62e-7ec933df7bb6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ZCA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60000/60000 [00:14<00:00, 4004.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ZCA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 4663.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'kornia.enhance.zca.ZCAWhitening'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231211_023514-6gk9y3h8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/simon-team-777/TEST/runs/6gk9y3h8' target=\"_blank\">noble-grass-3</a></strong> to <a href='https://wandb.ai/simon-team-777/TEST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/simon-team-777/TEST' target=\"_blank\">https://wandb.ai/simon-team-777/TEST</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/simon-team-777/TEST/runs/6gk9y3h8' target=\"_blank\">https://wandb.ai/simon-team-777/TEST/runs/6gk9y3h8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyper-parameters: \n",
            " {'dataset': 'MNIST', 'subset': 'imagenette', 'model': 'ConvNet', 'ipc': 5, 'eval_mode': 'S', 'num_eval': 3, 'eval_it': 100, 'epoch_eval_train': 1000, 'Iteration': 300, 'lr_img': 1000, 'lr_teacher': 0.01, 'lr_init': 0.01, 'batch_real': 256, 'batch_syn': 50, 'batch_train': 256, 'pix_init': 'samples_predicted_correctly', 'dsa': True, 'dsa_strategy': 'color_crop_cutout_flip_scale_rotate', 'dc_aug_param': None, 'dsa_param': <__main__.ParamDiffAug object at 0x7a7fef8e9e10>, 'data_path': './data/', 'buffer_path': './buffers/', 'expert_epochs': 2, 'syn_steps': 80, 'max_start_epoch': 5, 'min_start_epoch': 0, 'zca': True, 'load_all': False, 'no_aug': False, 'texture': False, 'canvas_size': 2, 'canvas_samples': 1, 'max_files': None, 'max_experts': None, 'force_save': False, 'ema_decay': 0.999, 'lr_y': 5.0, 'Momentum_y': 0.9, 'project': 'TEST', 'threshold': 1.0, 'record_loss': False, 'Sequential_Generation': True, 'expansion_end_epoch': 3000, 'current_max_start_epoch': 1, 'skip_first_eva': True, 'parall_eva': False, 'lr_lr': 1e-05, 'res': 32, 'device': 'cuda', 'Initialize_Label_With_Another_Model': False, 'Initialize_Label_Model': '', 'Initialize_Label_Model_Dir': '', 'Label_Model_Timestamp': -1, 'zca_trans': ZCAWhitening(), 'im_size': [28, 28], '_wandb': {}, 'distributed': False}\n",
            "Evaluation model pool:  ['ConvNet']\n",
            "BUILDING DATASET\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/60000 [00:00<?, ?it/s]<ipython-input-60-64cdc7cab3bc>:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels_all.append(class_map[torch.tensor(sample[1]).item()])\n",
            "100%|██████████| 60000/60000 [00:01<00:00, 45123.91it/s]\n",
            "60000it [00:00, 747328.06it/s]\n",
            "<ipython-input-60-64cdc7cab3bc>:90: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1) # [0,0,0, 1,1,1, ..., 9,9,9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class c = 0: 5923 real images\n",
            "class c = 1: 6742 real images\n",
            "class c = 2: 5958 real images\n",
            "class c = 3: 6131 real images\n",
            "class c = 4: 5842 real images\n",
            "class c = 5: 5421 real images\n",
            "class c = 6: 5918 real images\n",
            "class c = 7: 6265 real images\n",
            "class c = 8: 5851 real images\n",
            "class c = 9: 5949 real images\n",
            "real images channel 0, mean = 0.0000, std = 0.5890\n",
            "Expert Dir: ./buffers/MNIST/ConvNet\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "0.0.0\n",
            "1.0.0\n",
            "2.0.0\n",
            "3.0.0\n",
            "4.0.0\n",
            "5.0.0\n",
            "6.0.0\n",
            "7.0.0\n",
            "8.0.0\n",
            "9.0.0\n",
            "[2023-12-11 02:35:18] training begins\n",
            "InitialAcc:1.0\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:35:21] iter = 0000, loss = 0.7283\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "[2023-12-11 02:35:46] iter = 0010, loss = 0.5277\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:36:11] iter = 0020, loss = 0.4558\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:36:35] iter = 0030, loss = 0.4289\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:37:00] iter = 0040, loss = 0.4136\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:37:25] iter = 0050, loss = 0.3819\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:37:50] iter = 0060, loss = 0.3770\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:38:15] iter = 0070, loss = 0.3694\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:38:40] iter = 0080, loss = 0.3583\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:39:06] iter = 0090, loss = 0.3344\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 100\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'batchmode': False, 'latestseed': -1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 113.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:39:37] Evaluate_00: epoch = 1000 train time = 8 s train loss = 0.023409 train acc = 0.0200, test acc = 0.9053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 112.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:39:46] Evaluate_01: epoch = 1000 train time = 8 s train loss = 0.024020 train acc = 0.0200, test acc = 0.9191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 115.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:39:55] Evaluate_02: epoch = 1000 train time = 8 s train loss = 0.022672 train acc = 0.0200, test acc = 0.9083\n",
            "Evaluate 3 random ConvNet, mean = 0.9109 std = 0.0059\n",
            "-------------------------\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:39:58] iter = 0100, loss = 0.3264\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "[2023-12-11 02:40:23] iter = 0110, loss = 0.3330\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:40:49] iter = 0120, loss = 0.3168\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:41:14] iter = 0130, loss = 0.3120\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:41:39] iter = 0140, loss = 0.3041\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:42:04] iter = 0150, loss = 0.3021\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:42:29] iter = 0160, loss = 0.2924\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:42:54] iter = 0170, loss = 0.3022\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:43:20] iter = 0180, loss = 0.3025\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:43:45] iter = 0190, loss = 0.2883\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 200\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'batchmode': False, 'latestseed': -1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 112.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:44:17] Evaluate_00: epoch = 1000 train time = 8 s train loss = 0.018289 train acc = 0.0200, test acc = 0.9251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 113.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:44:25] Evaluate_01: epoch = 1000 train time = 8 s train loss = 0.017977 train acc = 0.0200, test acc = 0.9254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 113.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:44:34] Evaluate_02: epoch = 1000 train time = 8 s train loss = 0.016717 train acc = 0.0200, test acc = 0.9264\n",
            "Evaluate 3 random ConvNet, mean = 0.9256 std = 0.0006\n",
            "-------------------------\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:44:37] iter = 0200, loss = 0.2910\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:45:02] iter = 0210, loss = 0.2920\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:45:27] iter = 0220, loss = 0.2774\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:45:53] iter = 0230, loss = 0.2819\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:46:18] iter = 0240, loss = 0.2840\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "[2023-12-11 02:46:43] iter = 0250, loss = 0.2830\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:47:08] iter = 0260, loss = 0.2845\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:47:33] iter = 0270, loss = 0.2739\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:47:59] iter = 0280, loss = 0.2767\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "[2023-12-11 02:48:24] iter = 0290, loss = 0.2831\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_2.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_1.pt\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_3.pt\n",
            "-------------------------\n",
            "Evaluation\n",
            "model_train = ConvNet, model_eval = ConvNet, iteration = 300\n",
            "DSA augmentation strategy: \n",
            " color_crop_cutout_flip_scale_rotate\n",
            "DSA augmentation parameters: \n",
            " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'ratio_noise': 0.05, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5, 'batchmode': False, 'latestseed': -1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 111.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:48:56] Evaluate_00: epoch = 1000 train time = 8 s train loss = 0.032610 train acc = 0.0200, test acc = 0.9251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 117.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:49:04] Evaluate_01: epoch = 1000 train time = 8 s train loss = 0.015364 train acc = 0.0200, test acc = 0.9306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1001/1001 [00:08<00:00, 111.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2023-12-11 02:49:13] Evaluate_02: epoch = 1000 train time = 8 s train loss = 0.024546 train acc = 0.0200, test acc = 0.9211\n",
            "Evaluate 3 random ConvNet, mean = 0.9256 std = 0.0039\n",
            "-------------------------\n",
            "loading file ./buffers/MNIST/ConvNet/replay_buffer_0.pt\n",
            "[2023-12-11 02:49:16] iter = 0300, loss = 0.2763\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.633 MB of 0.633 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40edb8155df147fc9a290c444ccf380d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy/ConvNet</td><td>▁██</td></tr><tr><td>Grand_Loss</td><td>█▇▅▅▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▁▂▂▂▁▁▁▂▁▁</td></tr><tr><td>Max_Accuracy/ConvNet</td><td>▁██</td></tr><tr><td>Max_Std/ConvNet</td><td>█▁▁</td></tr><tr><td>Progress</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Start_Epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Std/ConvNet</td><td>█▁▅</td></tr><tr><td>Synthetic_LR</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy/ConvNet</td><td>0.9256</td></tr><tr><td>Grand_Loss</td><td>0.27631</td></tr><tr><td>Max_Accuracy/ConvNet</td><td>0.92563</td></tr><tr><td>Max_Std/ConvNet</td><td>0.00056</td></tr><tr><td>Progress</td><td>300</td></tr><tr><td>Start_Epoch</td><td>0</td></tr><tr><td>Std/ConvNet</td><td>0.00389</td></tr><tr><td>Synthetic_LR</td><td>0.0356</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">noble-grass-3</strong> at: <a href='https://wandb.ai/simon-team-777/TEST/runs/6gk9y3h8' target=\"_blank\">https://wandb.ai/simon-team-777/TEST/runs/6gk9y3h8</a><br/>Synced 5 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20231211_023514-6gk9y3h8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SQchFMq7LYF5",
        "PMLKc4KQLLvt",
        "5N6fzL9R10Xl",
        "q8akzbQFmPS-",
        "_IOjDySs2nyV"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40edb8155df147fc9a290c444ccf380d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_933241d07ed1412abc3613eb1a4e3ac3",
              "IPY_MODEL_b472a3f19456458394767b328f6412a2"
            ],
            "layout": "IPY_MODEL_b130aec329e845a58125a13a34a17efc"
          }
        },
        "933241d07ed1412abc3613eb1a4e3ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7484565cbfa40f68b7588f3ed51b8ee",
            "placeholder": "​",
            "style": "IPY_MODEL_f567b2c58bd841b2998bcfbeb0d7e6a3",
            "value": "0.673 MB of 0.673 MB uploaded\r"
          }
        },
        "b472a3f19456458394767b328f6412a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3aa5ce1591a4432a4cff5445d780176",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f80383b278a47f4b26629c32d9930a3",
            "value": 1
          }
        },
        "b130aec329e845a58125a13a34a17efc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7484565cbfa40f68b7588f3ed51b8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f567b2c58bd841b2998bcfbeb0d7e6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3aa5ce1591a4432a4cff5445d780176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f80383b278a47f4b26629c32d9930a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}